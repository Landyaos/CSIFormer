{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import hdf5storage\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理与数据集构建\n",
    "class MIMODataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    MIMO-OFDM 数据集加载器\n",
    "    维度说明：\n",
    "    - tx_signal: [data_size, n_subc, n_sym, n_tx, 2] (实虚分量)\n",
    "    - rx_signal: [data_size, n_subc, n_sym, n_rx, 2]\n",
    "    - csi:       [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    \"\"\"\n",
    "    def __init__(self, tx_signal, rx_signal, csi):\n",
    "        # 合并所有数据样本\n",
    "        self.data_size = tx_signal.shape[0]\n",
    "        self.tx_signal = tx_signal\n",
    "        self.rx_signal = rx_signal\n",
    "        self.csi = csi\n",
    "\n",
    "        # 维度校验\n",
    "        assert tx_signal.shape[:-2] == rx_signal.shape[:-2], \"数据维度不匹配\"\n",
    "        assert csi.shape[:-3] == tx_signal.shape[:-2], \"CSI维度不匹配\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.csi[idx],    # [n_subc, n_sym, n_tx, n_rx, 2] \n",
    "            self.rx_signal[idx], # [n_subc, n_sym, n_rx, 2]\n",
    "            self.tx_signal[idx]  # [n_subc, n_sym, n_tx, 2]\n",
    "        )\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"带预激活的残差块\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = F.gelu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return residual + x\n",
    "\n",
    "class SubcarrierAttention(nn.Module):\n",
    "    \"\"\"子载波级自注意力模块\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim*2, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, subc, embed_dim]\n",
    "        attn_out, _ = self.mha(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "class DNNResEQWithAttention(nn.Module):\n",
    "    def __init__(self, n_subc=224, n_sym=14, n_tx=2, n_rx=2, \n",
    "                 hidden_dim=256, num_blocks=6):\n",
    "        \"\"\"\n",
    "        参数说明:\n",
    "        - n_subc: 子载波数 (默认224)\n",
    "        - n_sym:  OFDM符号数 (默认14)\n",
    "        - n_tx:   发射天线数 (默认2)\n",
    "        - n_rx:   接收天线数 (默认2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_subc = n_subc\n",
    "        self.n_sym = n_sym\n",
    "        \n",
    "        # 输入特征维度计算 (CSI + RX)\n",
    "        csi_feat_dim = n_tx * n_rx * 2  # 每个CSI矩阵展平维度\n",
    "        rx_feat_dim = n_rx * 2\n",
    "        input_dim = csi_feat_dim + rx_feat_dim\n",
    "        \n",
    "        # 输入预处理\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # 子载波注意力编码器\n",
    "        self.subc_attention = SubcarrierAttention(hidden_dim)\n",
    "        \n",
    "        # 时空残差块\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                ResidualBlock(hidden_dim, hidden_dim*2),\n",
    "                SubcarrierAttention(hidden_dim) if i%2==0 else nn.Identity()\n",
    "            ) for i in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # 输出重建层\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, n_tx*2),  # 每个发射天线的实虚部\n",
    "            nn.Tanh()  # 约束输出范围\n",
    "        )\n",
    "\n",
    "    def forward(self, csi, rx_signal):\n",
    "        \"\"\"\n",
    "        输入维度:\n",
    "        - csi: [batch, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        - rx_signal: [batch, n_subc, n_sym, n_rx, 2]\n",
    "        \n",
    "        输出维度: \n",
    "        [batch, n_subc, n_sym, n_tx, 2]\n",
    "        \"\"\"\n",
    "        batch_size = csi.size(0)\n",
    "        \n",
    "        # 特征展平处理 ------------------------------------------\n",
    "        # CSI特征: [batch, subc, sym, tx, rx, 2] => [batch, subc, sym, tx*rx*2]\n",
    "        csi_flat = csi.view(*csi.shape[:3], -1)  \n",
    "        \n",
    "        # RX特征: [batch, subc, sym, rx, 2] => [batch, subc, sym, rx*2]\n",
    "        rx_flat = rx_signal.view(*rx_signal.shape[:3], -1)\n",
    "        \n",
    "        # 合并特征: [batch, subc, sym, (tx*rx + rx)*2]\n",
    "        x = torch.cat([csi_flat, rx_flat], dim=-1)\n",
    "        \n",
    "        # 维度重组为: [batch*sym, subc, features]\n",
    "        x = x.permute(0, 2, 1, 3)  # [batch, sym, subc, features]\n",
    "        x = x.reshape(batch_size*self.n_sym, self.n_subc, -1)\n",
    "        \n",
    "        # 特征投影 ----------------------------------------------\n",
    "        x = self.input_proj(x)  # [batch*sym, subc, hidden_dim]\n",
    "        \n",
    "        # 子载波级注意力编码 -------------------------------------\n",
    "        x = self.subc_attention(x)  # 保持维度 [batch*sym, subc, hidden]\n",
    "        \n",
    "        # 残差块处理 ---------------------------------------------\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)  # 每个块处理都保持维度\n",
    "            \n",
    "        # 输出重建 -----------------------------------------------\n",
    "        # 每个子载波独立输出\n",
    "        output = self.output_layer(x)  # [batch*sym, subc, n_tx*2]\n",
    "        \n",
    "        # 维度恢复\n",
    "        output = output.view(batch_size, self.n_sym, self.n_subc, -1)\n",
    "        output = output.permute(0, 2, 1, 3)  # [batch, subc, sym, n_tx*2]\n",
    "        \n",
    "        # 重塑为最终输出格式\n",
    "        return output.view(batch_size, self.n_subc, self.n_sym, -1, 2)\n",
    "\n",
    "def dataset_preprocess(data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    tx_signal = torch.tensor(data['txSignalData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, 2]\n",
    "    rx_signal = torch.tensor(data['rxSignalData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_rx, 2]\n",
    "    csi = torch.tensor(data['csiLabelData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return MIMODataset(tx_signal, rx_signal, csi)\n",
    "\n",
    "class ComplexMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param alpha: 第一部分损失的权重\n",
    "        :param beta:  第二部分损失的权重\n",
    "        \"\"\"\n",
    "        super(ComplexMSELoss, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        复数信道估计的均方误差 (MSE) 损失函数。\n",
    "        x_py: (batch_size, csi_matrix, 2)，估计值\n",
    "        y_py: (batch_size, csi_matrix, 2)，真实值\n",
    "        \"\"\"\n",
    "        diff = output - target  # 差值，形状保持一致\n",
    "        loss = torch.mean(diff[..., 0]**2 + diff[..., 1]**2)  # 实部和虚部平方和\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir='./checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    model.to(device)\n",
    "    # 查看是否有可用的最近 checkpoint\n",
    "    latest_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_v1_latest.pth')\n",
    "    best_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_v1_best.pth')\n",
    "\n",
    "    if os.path.isfile(latest_path):\n",
    "        print(f\"[INFO] Resuming training from '{latest_path}'\")\n",
    "        checkpoint = torch.load(latest_path, map_location=device)\n",
    "\n",
    "        # 加载模型、优化器、调度器状态\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', best_loss)\n",
    "        print(f\"[INFO] Resumed epoch {start_epoch}, best_loss={best_loss:.6f}\")\n",
    "    \n",
    "    # 分epoch训练\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{epochs}]\")\n",
    "        # --------------------- Train ---------------------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (csi, rx_signal, tx_signal) in enumerate(dataloader_train):\n",
    "            csi = csi.to(device)\n",
    "            rx_signal = rx_signal.to(device)\n",
    "            tx_signal = tx_signal.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(csi, rx_signal)\n",
    "            loss = criterion(output, tx_signal)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader_train)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = total_loss / len(dataloader_train)\n",
    "        # 学习率调度器步进（根据策略）\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(train_loss)  # 对于 ReduceLROnPlateau 等需要传入指标的调度器\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader_train)}\")\n",
    "\n",
    "        # --------------------- Validate ---------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (csi, rx_signal, tx_signal) in enumerate(dataloader_val):\n",
    "                csi = csi.to(device)\n",
    "                rx_signal = rx_signal.to(device)\n",
    "                tx_signal = tx_signal.to(device)\n",
    "                output = model(csi, rx_signal)\n",
    "                loss = criterion(output, tx_signal)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(dataloader_val)\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # --------------------- Checkpoint 保存 ---------------------\n",
    "        # 1) 保存最新checkpoint（确保断点续训）\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "            'best_loss': best_loss,\n",
    "        }, latest_path)\n",
    "\n",
    "        # 2) 如果当前验证集 Loss 最佳，则保存为 best.pth\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, best_path)\n",
    "            print(f\"[INFO] Best model saved at epoch {epoch + 1}, val_loss={val_loss:.4f}\")\n",
    "        # 3) 每隔5个epoch保存当前epoch的权重\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, os.path.join(checkpoint_dir, model.__class__.__name__ + '_epoch_'+str(epoch)+'.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "load done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"load data\")\n",
    "data_train = hdf5storage.loadmat('./data/raw/trainData.mat')\n",
    "data_val = hdf5storage.loadmat('./data/raw/valData.mat')\n",
    "print(\"load done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_preprocess(data_train)\n",
    "dataset_val = dataset_preprocess(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 3693852\n",
      "train model\n"
     ]
    }
   ],
   "source": [
    "model = DNNResEQWithAttention()\n",
    "# 计算参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {count_parameters(model)}\")\n",
    "print('train model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 主函数执行\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "lr = 1e-3\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "shuffle_flag = True\n",
    "criterion = ComplexMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "dataloader_val = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20]\n",
      "Epoch 1, Batch 50/2400, Loss: 1.4318\n",
      "Epoch 1, Batch 100/2400, Loss: 0.8257\n",
      "Epoch 1, Batch 150/2400, Loss: 0.7361\n",
      "Epoch 1, Batch 200/2400, Loss: 0.6839\n",
      "Epoch 1, Batch 250/2400, Loss: 0.6798\n",
      "Epoch 1, Batch 300/2400, Loss: 0.6276\n",
      "Epoch 1, Batch 350/2400, Loss: 0.5007\n",
      "Epoch 1, Batch 400/2400, Loss: 0.1692\n",
      "Epoch 1, Batch 450/2400, Loss: 0.1357\n",
      "Epoch 1, Batch 500/2400, Loss: 0.0793\n",
      "Epoch 1, Batch 550/2400, Loss: 0.1014\n",
      "Epoch 1, Batch 600/2400, Loss: 0.0587\n",
      "Epoch 1, Batch 650/2400, Loss: 0.0449\n",
      "Epoch 1, Batch 700/2400, Loss: 0.0721\n",
      "Epoch 1, Batch 750/2400, Loss: 0.0491\n",
      "Epoch 1, Batch 800/2400, Loss: 0.0517\n",
      "Epoch 1, Batch 850/2400, Loss: 0.0493\n",
      "Epoch 1, Batch 900/2400, Loss: 0.0439\n",
      "Epoch 1, Batch 950/2400, Loss: 0.0456\n",
      "Epoch 1, Batch 1000/2400, Loss: 0.0465\n",
      "Epoch 1, Batch 1050/2400, Loss: 0.0336\n",
      "Epoch 1, Batch 1100/2400, Loss: 0.0432\n",
      "Epoch 1, Batch 1150/2400, Loss: 0.0320\n",
      "Epoch 1, Batch 1200/2400, Loss: 0.0357\n",
      "Epoch 1, Batch 1250/2400, Loss: 0.0371\n",
      "Epoch 1, Batch 1300/2400, Loss: 0.0253\n",
      "Epoch 1, Batch 1350/2400, Loss: 0.0247\n",
      "Epoch 1, Batch 1400/2400, Loss: 0.0258\n",
      "Epoch 1, Batch 1450/2400, Loss: 0.0191\n",
      "Epoch 1, Batch 1500/2400, Loss: 0.0269\n",
      "Epoch 1, Batch 1550/2400, Loss: 0.0382\n",
      "Epoch 1, Batch 1600/2400, Loss: 0.0216\n",
      "Epoch 1, Batch 1650/2400, Loss: 0.0208\n",
      "Epoch 1, Batch 1700/2400, Loss: 0.0136\n",
      "Epoch 1, Batch 1750/2400, Loss: 0.0244\n",
      "Epoch 1, Batch 1800/2400, Loss: 0.0243\n",
      "Epoch 1, Batch 1850/2400, Loss: 0.0201\n",
      "Epoch 1, Batch 1900/2400, Loss: 0.0269\n",
      "Epoch 1, Batch 1950/2400, Loss: 0.0206\n",
      "Epoch 1, Batch 2000/2400, Loss: 0.0158\n",
      "Epoch 1, Batch 2050/2400, Loss: 0.0282\n",
      "Epoch 1, Batch 2100/2400, Loss: 0.0177\n",
      "Epoch 1, Batch 2150/2400, Loss: 0.0122\n",
      "Epoch 1, Batch 2200/2400, Loss: 0.0186\n",
      "Epoch 1, Batch 2250/2400, Loss: 0.0152\n",
      "Epoch 1, Batch 2300/2400, Loss: 0.0113\n",
      "Epoch 1, Batch 2350/2400, Loss: 0.0232\n",
      "Epoch 1, Batch 2400/2400, Loss: 0.0285\n",
      "Epoch 1, Loss: 0.1639847676681044\n",
      "Val Loss: 0.0182\n",
      "[INFO] Best model saved at epoch 1, val_loss=0.0182\n",
      "\n",
      "Epoch [2/20]\n",
      "Epoch 2, Batch 50/2400, Loss: 0.0279\n",
      "Epoch 2, Batch 100/2400, Loss: 0.0161\n",
      "Epoch 2, Batch 150/2400, Loss: 0.0216\n",
      "Epoch 2, Batch 200/2400, Loss: 0.0191\n",
      "Epoch 2, Batch 250/2400, Loss: 0.0197\n",
      "Epoch 2, Batch 300/2400, Loss: 0.0229\n",
      "Epoch 2, Batch 350/2400, Loss: 0.0228\n",
      "Epoch 2, Batch 400/2400, Loss: 0.0164\n",
      "Epoch 2, Batch 450/2400, Loss: 0.0310\n",
      "Epoch 2, Batch 500/2400, Loss: 0.0239\n",
      "Epoch 2, Batch 550/2400, Loss: 0.0278\n",
      "Epoch 2, Batch 600/2400, Loss: 0.0311\n",
      "Epoch 2, Batch 650/2400, Loss: 0.0203\n",
      "Epoch 2, Batch 700/2400, Loss: 0.0218\n",
      "Epoch 2, Batch 750/2400, Loss: 0.0238\n",
      "Epoch 2, Batch 800/2400, Loss: 0.0199\n",
      "Epoch 2, Batch 850/2400, Loss: 0.0217\n",
      "Epoch 2, Batch 900/2400, Loss: 0.0107\n",
      "Epoch 2, Batch 950/2400, Loss: 0.0196\n",
      "Epoch 2, Batch 1000/2400, Loss: 0.0176\n",
      "Epoch 2, Batch 1050/2400, Loss: 0.0125\n",
      "Epoch 2, Batch 1100/2400, Loss: 0.0218\n",
      "Epoch 2, Batch 1150/2400, Loss: 0.0091\n",
      "Epoch 2, Batch 1200/2400, Loss: 0.0125\n",
      "Epoch 2, Batch 1250/2400, Loss: 0.0169\n",
      "Epoch 2, Batch 1300/2400, Loss: 0.0097\n",
      "Epoch 2, Batch 1350/2400, Loss: 0.0195\n",
      "Epoch 2, Batch 1400/2400, Loss: 0.0165\n",
      "Epoch 2, Batch 1450/2400, Loss: 0.0213\n",
      "Epoch 2, Batch 1500/2400, Loss: 0.0124\n",
      "Epoch 2, Batch 1550/2400, Loss: 0.0170\n",
      "Epoch 2, Batch 1600/2400, Loss: 0.0163\n",
      "Epoch 2, Batch 1650/2400, Loss: 0.0095\n",
      "Epoch 2, Batch 1700/2400, Loss: 0.0119\n",
      "Epoch 2, Batch 1750/2400, Loss: 0.0178\n",
      "Epoch 2, Batch 1800/2400, Loss: 0.0187\n",
      "Epoch 2, Batch 1850/2400, Loss: 0.0158\n",
      "Epoch 2, Batch 1900/2400, Loss: 0.0080\n",
      "Epoch 2, Batch 1950/2400, Loss: 0.0280\n",
      "Epoch 2, Batch 2000/2400, Loss: 0.0140\n",
      "Epoch 2, Batch 2050/2400, Loss: 0.0117\n",
      "Epoch 2, Batch 2100/2400, Loss: 0.0114\n",
      "Epoch 2, Batch 2150/2400, Loss: 0.0103\n",
      "Epoch 2, Batch 2200/2400, Loss: 0.0172\n",
      "Epoch 2, Batch 2250/2400, Loss: 0.0115\n",
      "Epoch 2, Batch 2300/2400, Loss: 0.0163\n",
      "Epoch 2, Batch 2350/2400, Loss: 0.0175\n",
      "Epoch 2, Batch 2400/2400, Loss: 0.0256\n",
      "Epoch 2, Loss: 0.016782253984323082\n",
      "Val Loss: 0.0142\n",
      "[INFO] Best model saved at epoch 2, val_loss=0.0142\n",
      "\n",
      "Epoch [3/20]\n",
      "Epoch 3, Batch 50/2400, Loss: 0.0182\n",
      "Epoch 3, Batch 100/2400, Loss: 0.0162\n",
      "Epoch 3, Batch 150/2400, Loss: 0.0173\n",
      "Epoch 3, Batch 200/2400, Loss: 0.0200\n",
      "Epoch 3, Batch 250/2400, Loss: 0.0073\n",
      "Epoch 3, Batch 300/2400, Loss: 0.0203\n",
      "Epoch 3, Batch 350/2400, Loss: 0.0149\n",
      "Epoch 3, Batch 400/2400, Loss: 0.0114\n",
      "Epoch 3, Batch 450/2400, Loss: 0.0136\n",
      "Epoch 3, Batch 500/2400, Loss: 0.0081\n",
      "Epoch 3, Batch 550/2400, Loss: 0.0131\n",
      "Epoch 3, Batch 600/2400, Loss: 0.0135\n",
      "Epoch 3, Batch 650/2400, Loss: 0.0171\n",
      "Epoch 3, Batch 700/2400, Loss: 0.0210\n",
      "Epoch 3, Batch 750/2400, Loss: 0.0118\n",
      "Epoch 3, Batch 800/2400, Loss: 0.0094\n",
      "Epoch 3, Batch 850/2400, Loss: 0.0128\n",
      "Epoch 3, Batch 900/2400, Loss: 0.0112\n",
      "Epoch 3, Batch 950/2400, Loss: 0.0118\n",
      "Epoch 3, Batch 1000/2400, Loss: 0.0094\n",
      "Epoch 3, Batch 1050/2400, Loss: 0.0091\n",
      "Epoch 3, Batch 1100/2400, Loss: 0.0084\n",
      "Epoch 3, Batch 1150/2400, Loss: 0.0193\n",
      "Epoch 3, Batch 1200/2400, Loss: 0.0140\n",
      "Epoch 3, Batch 1250/2400, Loss: 0.0084\n",
      "Epoch 3, Batch 1300/2400, Loss: 0.0150\n",
      "Epoch 3, Batch 1350/2400, Loss: 0.0157\n",
      "Epoch 3, Batch 1400/2400, Loss: 0.0100\n",
      "Epoch 3, Batch 1450/2400, Loss: 0.0210\n",
      "Epoch 3, Batch 1500/2400, Loss: 0.0141\n",
      "Epoch 3, Batch 1550/2400, Loss: 0.0199\n",
      "Epoch 3, Batch 1600/2400, Loss: 0.0160\n",
      "Epoch 3, Batch 1650/2400, Loss: 0.0100\n",
      "Epoch 3, Batch 1700/2400, Loss: 0.0170\n",
      "Epoch 3, Batch 1750/2400, Loss: 0.0122\n",
      "Epoch 3, Batch 1800/2400, Loss: 0.0197\n",
      "Epoch 3, Batch 1850/2400, Loss: 0.0070\n",
      "Epoch 3, Batch 1900/2400, Loss: 0.0061\n",
      "Epoch 3, Batch 1950/2400, Loss: 0.0129\n",
      "Epoch 3, Batch 2000/2400, Loss: 0.0095\n",
      "Epoch 3, Batch 2050/2400, Loss: 0.0089\n",
      "Epoch 3, Batch 2100/2400, Loss: 0.0072\n",
      "Epoch 3, Batch 2150/2400, Loss: 0.0052\n",
      "Epoch 3, Batch 2200/2400, Loss: 0.0183\n",
      "Epoch 3, Batch 2250/2400, Loss: 0.0149\n",
      "Epoch 3, Batch 2300/2400, Loss: 0.0077\n",
      "Epoch 3, Batch 2350/2400, Loss: 0.0056\n",
      "Epoch 3, Batch 2400/2400, Loss: 0.0230\n",
      "Epoch 3, Loss: 0.013463112991982294\n",
      "Val Loss: 0.0092\n",
      "[INFO] Best model saved at epoch 3, val_loss=0.0092\n",
      "\n",
      "Epoch [4/20]\n",
      "Epoch 4, Batch 50/2400, Loss: 0.0131\n",
      "Epoch 4, Batch 100/2400, Loss: 0.0124\n",
      "Epoch 4, Batch 150/2400, Loss: 0.5448\n",
      "Epoch 4, Batch 200/2400, Loss: 0.4388\n",
      "Epoch 4, Batch 250/2400, Loss: 0.2446\n",
      "Epoch 4, Batch 300/2400, Loss: 0.1285\n",
      "Epoch 4, Batch 350/2400, Loss: 0.0569\n",
      "Epoch 4, Batch 400/2400, Loss: 0.0508\n",
      "Epoch 4, Batch 450/2400, Loss: 0.0354\n",
      "Epoch 4, Batch 500/2400, Loss: 0.0422\n",
      "Epoch 4, Batch 550/2400, Loss: 0.0207\n",
      "Epoch 4, Batch 600/2400, Loss: 0.0350\n",
      "Epoch 4, Batch 650/2400, Loss: 0.0230\n",
      "Epoch 4, Batch 700/2400, Loss: 0.0217\n",
      "Epoch 4, Batch 750/2400, Loss: 0.0135\n",
      "Epoch 4, Batch 800/2400, Loss: 0.0119\n",
      "Epoch 4, Batch 850/2400, Loss: 0.0173\n",
      "Epoch 4, Batch 900/2400, Loss: 0.0245\n",
      "Epoch 4, Batch 950/2400, Loss: 0.0093\n",
      "Epoch 4, Batch 1000/2400, Loss: 0.0159\n",
      "Epoch 4, Batch 1050/2400, Loss: 0.0157\n",
      "Epoch 4, Batch 1100/2400, Loss: 0.0129\n",
      "Epoch 4, Batch 1150/2400, Loss: 0.0141\n",
      "Epoch 4, Batch 1200/2400, Loss: 0.0108\n",
      "Epoch 4, Batch 1250/2400, Loss: 0.0144\n",
      "Epoch 4, Batch 1300/2400, Loss: 0.0136\n",
      "Epoch 4, Batch 1350/2400, Loss: 0.0127\n",
      "Epoch 4, Batch 1400/2400, Loss: 0.0134\n",
      "Epoch 4, Batch 1450/2400, Loss: 0.0191\n",
      "Epoch 4, Batch 1500/2400, Loss: 0.0104\n",
      "Epoch 4, Batch 1550/2400, Loss: 0.0073\n",
      "Epoch 4, Batch 1600/2400, Loss: 0.0151\n",
      "Epoch 4, Batch 1650/2400, Loss: 0.0079\n",
      "Epoch 4, Batch 1700/2400, Loss: 0.0114\n",
      "Epoch 4, Batch 1750/2400, Loss: 0.0089\n",
      "Epoch 4, Batch 1800/2400, Loss: 0.0132\n",
      "Epoch 4, Batch 1850/2400, Loss: 0.0087\n",
      "Epoch 4, Batch 1900/2400, Loss: 0.0059\n",
      "Epoch 4, Batch 1950/2400, Loss: 0.0108\n",
      "Epoch 4, Batch 2000/2400, Loss: 0.0066\n",
      "Epoch 4, Batch 2050/2400, Loss: 0.0121\n",
      "Epoch 4, Batch 2100/2400, Loss: 0.0086\n",
      "Epoch 4, Batch 2150/2400, Loss: 0.0173\n",
      "Epoch 4, Batch 2200/2400, Loss: 0.0152\n",
      "Epoch 4, Batch 2250/2400, Loss: 0.0110\n",
      "Epoch 4, Batch 2300/2400, Loss: 0.0043\n",
      "Epoch 4, Batch 2350/2400, Loss: 0.0216\n",
      "Epoch 4, Batch 2400/2400, Loss: 0.0236\n",
      "Epoch 4, Loss: 0.043740505670624166\n",
      "Val Loss: 0.0115\n",
      "\n",
      "Epoch [5/20]\n",
      "Epoch 5, Batch 50/2400, Loss: 0.0173\n",
      "Epoch 5, Batch 100/2400, Loss: 0.0129\n",
      "Epoch 5, Batch 150/2400, Loss: 0.0116\n",
      "Epoch 5, Batch 200/2400, Loss: 0.0204\n",
      "Epoch 5, Batch 250/2400, Loss: 0.0176\n",
      "Epoch 5, Batch 300/2400, Loss: 0.0125\n",
      "Epoch 5, Batch 350/2400, Loss: 0.0163\n",
      "Epoch 5, Batch 400/2400, Loss: 0.0115\n",
      "Epoch 5, Batch 450/2400, Loss: 0.0130\n",
      "Epoch 5, Batch 500/2400, Loss: 0.0079\n",
      "Epoch 5, Batch 550/2400, Loss: 0.0138\n",
      "Epoch 5, Batch 600/2400, Loss: 0.0051\n",
      "Epoch 5, Batch 650/2400, Loss: 0.0200\n",
      "Epoch 5, Batch 700/2400, Loss: 0.0224\n",
      "Epoch 5, Batch 750/2400, Loss: 0.0169\n",
      "Epoch 5, Batch 800/2400, Loss: 0.0045\n",
      "Epoch 5, Batch 850/2400, Loss: 0.0166\n",
      "Epoch 5, Batch 900/2400, Loss: 0.0149\n",
      "Epoch 5, Batch 950/2400, Loss: 0.0076\n",
      "Epoch 5, Batch 1000/2400, Loss: 0.0225\n",
      "Epoch 5, Batch 1050/2400, Loss: 0.0158\n",
      "Epoch 5, Batch 1100/2400, Loss: 0.0073\n",
      "Epoch 5, Batch 1150/2400, Loss: 0.0071\n",
      "Epoch 5, Batch 1200/2400, Loss: 0.0127\n",
      "Epoch 5, Batch 1250/2400, Loss: 0.0126\n",
      "Epoch 5, Batch 1300/2400, Loss: 0.0059\n",
      "Epoch 5, Batch 1350/2400, Loss: 0.0123\n",
      "Epoch 5, Batch 1400/2400, Loss: 0.0093\n",
      "Epoch 5, Batch 1450/2400, Loss: 0.0089\n",
      "Epoch 5, Batch 1500/2400, Loss: 0.0085\n",
      "Epoch 5, Batch 1550/2400, Loss: 0.0112\n",
      "Epoch 5, Batch 1600/2400, Loss: 0.0117\n",
      "Epoch 5, Batch 1650/2400, Loss: 0.0110\n",
      "Epoch 5, Batch 1700/2400, Loss: 0.0051\n",
      "Epoch 5, Batch 1750/2400, Loss: 0.0125\n",
      "Epoch 5, Batch 1800/2400, Loss: 0.0100\n",
      "Epoch 5, Batch 1850/2400, Loss: 0.0060\n",
      "Epoch 5, Batch 1900/2400, Loss: 0.0136\n",
      "Epoch 5, Batch 1950/2400, Loss: 0.0087\n",
      "Epoch 5, Batch 2000/2400, Loss: 0.0065\n",
      "Epoch 5, Batch 2050/2400, Loss: 0.0157\n",
      "Epoch 5, Batch 2100/2400, Loss: 0.0109\n",
      "Epoch 5, Batch 2150/2400, Loss: 0.0097\n",
      "Epoch 5, Batch 2200/2400, Loss: 0.0115\n",
      "Epoch 5, Batch 2250/2400, Loss: 0.0096\n",
      "Epoch 5, Batch 2300/2400, Loss: 0.0071\n",
      "Epoch 5, Batch 2350/2400, Loss: 0.0056\n",
      "Epoch 5, Batch 2400/2400, Loss: 0.0161\n",
      "Epoch 5, Loss: 0.011744299531565049\n",
      "Val Loss: 0.0118\n",
      "\n",
      "Epoch [6/20]\n",
      "Epoch 6, Batch 50/2400, Loss: 0.0128\n",
      "Epoch 6, Batch 100/2400, Loss: 0.0117\n",
      "Epoch 6, Batch 150/2400, Loss: 0.0090\n",
      "Epoch 6, Batch 200/2400, Loss: 0.0097\n",
      "Epoch 6, Batch 250/2400, Loss: 0.0082\n",
      "Epoch 6, Batch 300/2400, Loss: 0.0078\n",
      "Epoch 6, Batch 350/2400, Loss: 0.0065\n",
      "Epoch 6, Batch 400/2400, Loss: 0.0084\n",
      "Epoch 6, Batch 450/2400, Loss: 0.0104\n",
      "Epoch 6, Batch 500/2400, Loss: 0.0123\n",
      "Epoch 6, Batch 550/2400, Loss: 0.0057\n",
      "Epoch 6, Batch 600/2400, Loss: 0.0114\n",
      "Epoch 6, Batch 650/2400, Loss: 0.0143\n",
      "Epoch 6, Batch 700/2400, Loss: 0.0079\n",
      "Epoch 6, Batch 750/2400, Loss: 0.0081\n",
      "Epoch 6, Batch 800/2400, Loss: 0.0114\n",
      "Epoch 6, Batch 850/2400, Loss: 0.0076\n",
      "Epoch 6, Batch 900/2400, Loss: 0.0051\n",
      "Epoch 6, Batch 950/2400, Loss: 0.0093\n",
      "Epoch 6, Batch 1000/2400, Loss: 0.0090\n",
      "Epoch 6, Batch 1050/2400, Loss: 0.0127\n",
      "Epoch 6, Batch 1100/2400, Loss: 0.0060\n",
      "Epoch 6, Batch 1150/2400, Loss: 0.0096\n",
      "Epoch 6, Batch 1200/2400, Loss: 0.0031\n",
      "Epoch 6, Batch 1250/2400, Loss: 0.0091\n",
      "Epoch 6, Batch 1300/2400, Loss: 0.0129\n",
      "Epoch 6, Batch 1350/2400, Loss: 0.0148\n",
      "Epoch 6, Batch 1400/2400, Loss: 0.0105\n",
      "Epoch 6, Batch 1450/2400, Loss: 0.0123\n",
      "Epoch 6, Batch 1500/2400, Loss: 0.0121\n",
      "Epoch 6, Batch 1550/2400, Loss: 0.0077\n",
      "Epoch 6, Batch 1600/2400, Loss: 0.0040\n",
      "Epoch 6, Batch 1650/2400, Loss: 0.0082\n",
      "Epoch 6, Batch 1700/2400, Loss: 0.0135\n",
      "Epoch 6, Batch 1750/2400, Loss: 0.0128\n",
      "Epoch 6, Batch 1800/2400, Loss: 0.0146\n",
      "Epoch 6, Batch 1850/2400, Loss: 0.0084\n",
      "Epoch 6, Batch 1900/2400, Loss: 0.0074\n",
      "Epoch 6, Batch 1950/2400, Loss: 0.0147\n",
      "Epoch 6, Batch 2000/2400, Loss: 0.0064\n",
      "Epoch 6, Batch 2050/2400, Loss: 0.0082\n",
      "Epoch 6, Batch 2100/2400, Loss: 0.0114\n",
      "Epoch 6, Batch 2150/2400, Loss: 0.0053\n",
      "Epoch 6, Batch 2200/2400, Loss: 0.0147\n",
      "Epoch 6, Batch 2250/2400, Loss: 0.0111\n",
      "Epoch 6, Batch 2300/2400, Loss: 0.0035\n",
      "Epoch 6, Batch 2350/2400, Loss: 0.0152\n",
      "Epoch 6, Batch 2400/2400, Loss: 0.0120\n",
      "Epoch 6, Loss: 0.010562249473490132\n",
      "Val Loss: 0.0119\n",
      "\n",
      "Epoch [7/20]\n",
      "Epoch 7, Batch 50/2400, Loss: 0.0146\n",
      "Epoch 7, Batch 100/2400, Loss: 0.0047\n",
      "Epoch 7, Batch 150/2400, Loss: 0.0094\n",
      "Epoch 7, Batch 200/2400, Loss: 0.0067\n",
      "Epoch 7, Batch 250/2400, Loss: 0.0107\n",
      "Epoch 7, Batch 300/2400, Loss: 0.0039\n",
      "Epoch 7, Batch 350/2400, Loss: 0.0048\n",
      "Epoch 7, Batch 400/2400, Loss: 0.0058\n",
      "Epoch 7, Batch 450/2400, Loss: 0.0133\n",
      "Epoch 7, Batch 500/2400, Loss: 0.0072\n",
      "Epoch 7, Batch 550/2400, Loss: 0.0053\n",
      "Epoch 7, Batch 600/2400, Loss: 0.0053\n",
      "Epoch 7, Batch 650/2400, Loss: 0.0057\n",
      "Epoch 7, Batch 700/2400, Loss: 0.0079\n",
      "Epoch 7, Batch 750/2400, Loss: 0.0106\n",
      "Epoch 7, Batch 800/2400, Loss: 0.0102\n",
      "Epoch 7, Batch 850/2400, Loss: 0.0079\n",
      "Epoch 7, Batch 900/2400, Loss: 0.0095\n",
      "Epoch 7, Batch 950/2400, Loss: 0.0136\n",
      "Epoch 7, Batch 1000/2400, Loss: 0.0138\n",
      "Epoch 7, Batch 1050/2400, Loss: 0.0127\n",
      "Epoch 7, Batch 1100/2400, Loss: 0.0053\n",
      "Epoch 7, Batch 1150/2400, Loss: 0.0097\n",
      "Epoch 7, Batch 1200/2400, Loss: 0.0074\n",
      "Epoch 7, Batch 1250/2400, Loss: 0.0054\n",
      "Epoch 7, Batch 1300/2400, Loss: 0.0092\n",
      "Epoch 7, Batch 1350/2400, Loss: 0.0090\n",
      "Epoch 7, Batch 1400/2400, Loss: 0.0119\n",
      "Epoch 7, Batch 1450/2400, Loss: 0.0090\n",
      "Epoch 7, Batch 1500/2400, Loss: 0.0030\n",
      "Epoch 7, Batch 1550/2400, Loss: 0.0150\n",
      "Epoch 7, Batch 1600/2400, Loss: 0.0109\n",
      "Epoch 7, Batch 1650/2400, Loss: 0.0076\n",
      "Epoch 7, Batch 1700/2400, Loss: 0.0113\n",
      "Epoch 7, Batch 1750/2400, Loss: 0.0117\n",
      "Epoch 7, Batch 1800/2400, Loss: 0.0101\n",
      "Epoch 7, Batch 1850/2400, Loss: 0.0085\n",
      "Epoch 7, Batch 1900/2400, Loss: 0.0114\n",
      "Epoch 7, Batch 1950/2400, Loss: 0.0120\n",
      "Epoch 7, Batch 2000/2400, Loss: 0.0139\n",
      "Epoch 7, Batch 2050/2400, Loss: 0.0131\n",
      "Epoch 7, Batch 2100/2400, Loss: 0.0105\n",
      "Epoch 7, Batch 2150/2400, Loss: 0.0107\n",
      "Epoch 7, Batch 2200/2400, Loss: 0.0147\n",
      "Epoch 7, Batch 2250/2400, Loss: 0.0173\n",
      "Epoch 7, Batch 2300/2400, Loss: 0.0077\n",
      "Epoch 7, Batch 2350/2400, Loss: 0.0133\n",
      "Epoch 7, Batch 2400/2400, Loss: 0.0140\n",
      "Epoch 7, Loss: 0.009536987273701622\n",
      "Val Loss: 0.0101\n",
      "\n",
      "Epoch [8/20]\n",
      "Epoch 8, Batch 50/2400, Loss: 0.0127\n",
      "Epoch 8, Batch 100/2400, Loss: 0.0073\n",
      "Epoch 8, Batch 150/2400, Loss: 0.0075\n",
      "Epoch 8, Batch 200/2400, Loss: 0.0049\n",
      "Epoch 8, Batch 250/2400, Loss: 0.0150\n",
      "Epoch 8, Batch 300/2400, Loss: 0.0125\n",
      "Epoch 8, Batch 350/2400, Loss: 0.0096\n",
      "Epoch 8, Batch 400/2400, Loss: 0.0049\n",
      "Epoch 8, Batch 450/2400, Loss: 0.0228\n",
      "Epoch 8, Batch 500/2400, Loss: 0.0097\n",
      "Epoch 8, Batch 550/2400, Loss: 0.0092\n",
      "Epoch 8, Batch 600/2400, Loss: 0.0066\n",
      "Epoch 8, Batch 650/2400, Loss: 0.0072\n",
      "Epoch 8, Batch 700/2400, Loss: 0.0093\n",
      "Epoch 8, Batch 750/2400, Loss: 0.0071\n",
      "Epoch 8, Batch 800/2400, Loss: 0.0100\n",
      "Epoch 8, Batch 850/2400, Loss: 0.0064\n",
      "Epoch 8, Batch 900/2400, Loss: 0.0050\n",
      "Epoch 8, Batch 950/2400, Loss: 0.0064\n",
      "Epoch 8, Batch 1000/2400, Loss: 0.0135\n",
      "Epoch 8, Batch 1050/2400, Loss: 0.0038\n",
      "Epoch 8, Batch 1100/2400, Loss: 0.0060\n",
      "Epoch 8, Batch 1150/2400, Loss: 0.0114\n",
      "Epoch 8, Batch 1200/2400, Loss: 0.0126\n",
      "Epoch 8, Batch 1250/2400, Loss: 0.0071\n",
      "Epoch 8, Batch 1300/2400, Loss: 0.0103\n",
      "Epoch 8, Batch 1350/2400, Loss: 0.0072\n",
      "Epoch 8, Batch 1400/2400, Loss: 0.0075\n",
      "Epoch 8, Batch 1450/2400, Loss: 0.0061\n",
      "Epoch 8, Batch 1500/2400, Loss: 0.0119\n",
      "Epoch 8, Batch 1550/2400, Loss: 0.0110\n",
      "Epoch 8, Batch 1600/2400, Loss: 0.0120\n",
      "Epoch 8, Batch 1650/2400, Loss: 0.0044\n",
      "Epoch 8, Batch 1700/2400, Loss: 0.0108\n",
      "Epoch 8, Batch 1750/2400, Loss: 0.0086\n",
      "Epoch 8, Batch 1800/2400, Loss: 0.0095\n",
      "Epoch 8, Batch 1850/2400, Loss: 0.0086\n",
      "Epoch 8, Batch 1900/2400, Loss: 0.0067\n",
      "Epoch 8, Batch 1950/2400, Loss: 0.0064\n",
      "Epoch 8, Batch 2000/2400, Loss: 0.0071\n",
      "Epoch 8, Batch 2050/2400, Loss: 0.0087\n",
      "Epoch 8, Batch 2100/2400, Loss: 0.0054\n",
      "Epoch 8, Batch 2150/2400, Loss: 0.0121\n",
      "Epoch 8, Batch 2200/2400, Loss: 0.0110\n",
      "Epoch 8, Batch 2250/2400, Loss: 0.0096\n",
      "Epoch 8, Batch 2300/2400, Loss: 0.0126\n",
      "Epoch 8, Batch 2350/2400, Loss: 0.0066\n",
      "Epoch 8, Batch 2400/2400, Loss: 0.0061\n",
      "Epoch 8, Loss: 0.00871325448480396\n",
      "Val Loss: 0.0086\n",
      "[INFO] Best model saved at epoch 8, val_loss=0.0086\n",
      "\n",
      "Epoch [9/20]\n",
      "Epoch 9, Batch 50/2400, Loss: 0.0043\n",
      "Epoch 9, Batch 100/2400, Loss: 0.0084\n",
      "Epoch 9, Batch 150/2400, Loss: 0.0086\n",
      "Epoch 9, Batch 200/2400, Loss: 0.0105\n",
      "Epoch 9, Batch 250/2400, Loss: 0.0040\n",
      "Epoch 9, Batch 300/2400, Loss: 0.0072\n",
      "Epoch 9, Batch 350/2400, Loss: 0.0043\n",
      "Epoch 9, Batch 400/2400, Loss: 0.0065\n",
      "Epoch 9, Batch 450/2400, Loss: 0.0086\n",
      "Epoch 9, Batch 500/2400, Loss: 0.0090\n",
      "Epoch 9, Batch 550/2400, Loss: 0.0122\n",
      "Epoch 9, Batch 600/2400, Loss: 0.0056\n",
      "Epoch 9, Batch 650/2400, Loss: 0.0054\n",
      "Epoch 9, Batch 700/2400, Loss: 0.0123\n",
      "Epoch 9, Batch 750/2400, Loss: 0.0077\n",
      "Epoch 9, Batch 800/2400, Loss: 0.0098\n",
      "Epoch 9, Batch 850/2400, Loss: 0.0128\n",
      "Epoch 9, Batch 900/2400, Loss: 0.0129\n",
      "Epoch 9, Batch 950/2400, Loss: 0.0082\n",
      "Epoch 9, Batch 1000/2400, Loss: 0.0178\n",
      "Epoch 9, Batch 1050/2400, Loss: 0.0084\n",
      "Epoch 9, Batch 1100/2400, Loss: 0.0091\n",
      "Epoch 9, Batch 1150/2400, Loss: 0.0183\n",
      "Epoch 9, Batch 1200/2400, Loss: 0.0039\n",
      "Epoch 9, Batch 1250/2400, Loss: 0.0099\n",
      "Epoch 9, Batch 1300/2400, Loss: 0.0052\n",
      "Epoch 9, Batch 1350/2400, Loss: 0.0047\n",
      "Epoch 9, Batch 1400/2400, Loss: 0.0040\n",
      "Epoch 9, Batch 1450/2400, Loss: 0.0069\n",
      "Epoch 9, Batch 1500/2400, Loss: 0.0072\n",
      "Epoch 9, Batch 1550/2400, Loss: 0.0068\n",
      "Epoch 9, Batch 1600/2400, Loss: 0.0104\n",
      "Epoch 9, Batch 1650/2400, Loss: 0.0199\n",
      "Epoch 9, Batch 1700/2400, Loss: 0.0070\n",
      "Epoch 9, Batch 1750/2400, Loss: 0.0072\n",
      "Epoch 9, Batch 1800/2400, Loss: 0.0100\n",
      "Epoch 9, Batch 1850/2400, Loss: 0.0064\n",
      "Epoch 9, Batch 1900/2400, Loss: 0.0093\n",
      "Epoch 9, Batch 1950/2400, Loss: 0.0080\n",
      "Epoch 9, Batch 2000/2400, Loss: 0.0111\n",
      "Epoch 9, Batch 2050/2400, Loss: 0.0073\n",
      "Epoch 9, Batch 2100/2400, Loss: 0.0121\n",
      "Epoch 9, Batch 2150/2400, Loss: 0.0019\n",
      "Epoch 9, Batch 2200/2400, Loss: 0.0121\n",
      "Epoch 9, Batch 2250/2400, Loss: 0.0074\n",
      "Epoch 9, Batch 2300/2400, Loss: 0.0058\n",
      "Epoch 9, Batch 2350/2400, Loss: 0.0081\n",
      "Epoch 9, Batch 2400/2400, Loss: 0.0047\n",
      "Epoch 9, Loss: 0.008103648006023529\n",
      "Val Loss: 0.0111\n",
      "\n",
      "Epoch [10/20]\n",
      "Epoch 10, Batch 50/2400, Loss: 0.0123\n",
      "Epoch 10, Batch 100/2400, Loss: 0.0064\n",
      "Epoch 10, Batch 150/2400, Loss: 0.0121\n",
      "Epoch 10, Batch 200/2400, Loss: 0.0085\n",
      "Epoch 10, Batch 250/2400, Loss: 0.0118\n",
      "Epoch 10, Batch 300/2400, Loss: 0.0050\n",
      "Epoch 10, Batch 350/2400, Loss: 0.0080\n",
      "Epoch 10, Batch 400/2400, Loss: 0.0082\n",
      "Epoch 10, Batch 450/2400, Loss: 0.0044\n",
      "Epoch 10, Batch 500/2400, Loss: 0.0079\n",
      "Epoch 10, Batch 550/2400, Loss: 0.0154\n",
      "Epoch 10, Batch 600/2400, Loss: 0.0129\n",
      "Epoch 10, Batch 650/2400, Loss: 0.0119\n",
      "Epoch 10, Batch 700/2400, Loss: 0.0063\n",
      "Epoch 10, Batch 750/2400, Loss: 0.0041\n",
      "Epoch 10, Batch 800/2400, Loss: 0.0074\n",
      "Epoch 10, Batch 850/2400, Loss: 0.0046\n",
      "Epoch 10, Batch 900/2400, Loss: 0.0099\n",
      "Epoch 10, Batch 950/2400, Loss: 0.0070\n",
      "Epoch 10, Batch 1000/2400, Loss: 0.0095\n",
      "Epoch 10, Batch 1050/2400, Loss: 0.0065\n",
      "Epoch 10, Batch 1100/2400, Loss: 0.0044\n",
      "Epoch 10, Batch 1150/2400, Loss: 0.0095\n",
      "Epoch 10, Batch 1200/2400, Loss: 0.0070\n",
      "Epoch 10, Batch 1250/2400, Loss: 0.0071\n",
      "Epoch 10, Batch 1300/2400, Loss: 0.0074\n",
      "Epoch 10, Batch 1350/2400, Loss: 0.0071\n",
      "Epoch 10, Batch 1400/2400, Loss: 0.0055\n",
      "Epoch 10, Batch 1450/2400, Loss: 0.0061\n",
      "Epoch 10, Batch 1500/2400, Loss: 0.0077\n",
      "Epoch 10, Batch 1550/2400, Loss: 0.0051\n",
      "Epoch 10, Batch 1600/2400, Loss: 0.0094\n",
      "Epoch 10, Batch 1650/2400, Loss: 0.0138\n",
      "Epoch 10, Batch 1700/2400, Loss: 0.0077\n",
      "Epoch 10, Batch 1750/2400, Loss: 0.0063\n",
      "Epoch 10, Batch 1800/2400, Loss: 0.0072\n",
      "Epoch 10, Batch 1850/2400, Loss: 0.0076\n",
      "Epoch 10, Batch 1900/2400, Loss: 0.0053\n",
      "Epoch 10, Batch 1950/2400, Loss: 0.0101\n",
      "Epoch 10, Batch 2000/2400, Loss: 0.0133\n",
      "Epoch 10, Batch 2050/2400, Loss: 0.0091\n",
      "Epoch 10, Batch 2100/2400, Loss: 0.0102\n",
      "Epoch 10, Batch 2150/2400, Loss: 0.0087\n",
      "Epoch 10, Batch 2200/2400, Loss: 0.0086\n",
      "Epoch 10, Batch 2250/2400, Loss: 0.0111\n",
      "Epoch 10, Batch 2300/2400, Loss: 0.0120\n",
      "Epoch 10, Batch 2350/2400, Loss: 0.0038\n",
      "Epoch 10, Batch 2400/2400, Loss: 0.0061\n",
      "Epoch 10, Loss: 0.007691081814361193\n",
      "Val Loss: 0.0080\n",
      "[INFO] Best model saved at epoch 10, val_loss=0.0080\n",
      "\n",
      "Epoch [11/20]\n",
      "Epoch 11, Batch 50/2400, Loss: 0.0149\n",
      "Epoch 11, Batch 100/2400, Loss: 0.0107\n",
      "Epoch 11, Batch 150/2400, Loss: 0.0106\n",
      "Epoch 11, Batch 200/2400, Loss: 0.0096\n",
      "Epoch 11, Batch 250/2400, Loss: 0.0158\n",
      "Epoch 11, Batch 300/2400, Loss: 0.0087\n",
      "Epoch 11, Batch 350/2400, Loss: 0.0066\n",
      "Epoch 11, Batch 400/2400, Loss: 0.0032\n",
      "Epoch 11, Batch 450/2400, Loss: 0.0058\n",
      "Epoch 11, Batch 500/2400, Loss: 0.0040\n",
      "Epoch 11, Batch 550/2400, Loss: 0.0047\n",
      "Epoch 11, Batch 600/2400, Loss: 0.0071\n",
      "Epoch 11, Batch 650/2400, Loss: 0.0043\n",
      "Epoch 11, Batch 700/2400, Loss: 0.0058\n",
      "Epoch 11, Batch 750/2400, Loss: 0.0041\n",
      "Epoch 11, Batch 800/2400, Loss: 0.0070\n",
      "Epoch 11, Batch 850/2400, Loss: 0.0057\n",
      "Epoch 11, Batch 900/2400, Loss: 0.0064\n",
      "Epoch 11, Batch 950/2400, Loss: 0.0076\n",
      "Epoch 11, Batch 1000/2400, Loss: 0.0111\n",
      "Epoch 11, Batch 1050/2400, Loss: 0.0051\n",
      "Epoch 11, Batch 1100/2400, Loss: 0.0080\n",
      "Epoch 11, Batch 1150/2400, Loss: 0.0072\n",
      "Epoch 11, Batch 1200/2400, Loss: 0.0050\n",
      "Epoch 11, Batch 1250/2400, Loss: 0.0054\n",
      "Epoch 11, Batch 1300/2400, Loss: 0.0060\n",
      "Epoch 11, Batch 1350/2400, Loss: 0.0086\n",
      "Epoch 11, Batch 1400/2400, Loss: 0.0157\n",
      "Epoch 11, Batch 1450/2400, Loss: 0.0058\n",
      "Epoch 11, Batch 1500/2400, Loss: 0.0033\n",
      "Epoch 11, Batch 1550/2400, Loss: 0.0062\n",
      "Epoch 11, Batch 1600/2400, Loss: 0.0077\n",
      "Epoch 11, Batch 1650/2400, Loss: 0.0075\n",
      "Epoch 11, Batch 1700/2400, Loss: 0.0058\n",
      "Epoch 11, Batch 1750/2400, Loss: 0.0096\n",
      "Epoch 11, Batch 1800/2400, Loss: 0.0100\n",
      "Epoch 11, Batch 1850/2400, Loss: 0.0079\n",
      "Epoch 11, Batch 1900/2400, Loss: 0.0072\n",
      "Epoch 11, Batch 1950/2400, Loss: 0.0058\n",
      "Epoch 11, Batch 2000/2400, Loss: 0.0034\n",
      "Epoch 11, Batch 2050/2400, Loss: 0.0062\n",
      "Epoch 11, Batch 2100/2400, Loss: 0.0066\n",
      "Epoch 11, Batch 2150/2400, Loss: 0.0030\n",
      "Epoch 11, Batch 2200/2400, Loss: 0.0039\n",
      "Epoch 11, Batch 2250/2400, Loss: 0.0121\n",
      "Epoch 11, Batch 2300/2400, Loss: 0.0122\n",
      "Epoch 11, Batch 2350/2400, Loss: 0.0098\n",
      "Epoch 11, Batch 2400/2400, Loss: 0.0077\n",
      "Epoch 11, Loss: 0.007276833131666839\n",
      "Val Loss: 0.0088\n",
      "\n",
      "Epoch [12/20]\n",
      "Epoch 12, Batch 50/2400, Loss: 0.0056\n",
      "Epoch 12, Batch 100/2400, Loss: 0.0033\n",
      "Epoch 12, Batch 150/2400, Loss: 0.0019\n",
      "Epoch 12, Batch 200/2400, Loss: 0.0058\n",
      "Epoch 12, Batch 250/2400, Loss: 0.0058\n",
      "Epoch 12, Batch 300/2400, Loss: 0.0092\n",
      "Epoch 12, Batch 350/2400, Loss: 0.0099\n",
      "Epoch 12, Batch 400/2400, Loss: 0.0081\n",
      "Epoch 12, Batch 450/2400, Loss: 0.0092\n",
      "Epoch 12, Batch 500/2400, Loss: 0.0029\n",
      "Epoch 12, Batch 550/2400, Loss: 0.0068\n",
      "Epoch 12, Batch 600/2400, Loss: 0.0071\n",
      "Epoch 12, Batch 650/2400, Loss: 0.0043\n",
      "Epoch 12, Batch 700/2400, Loss: 0.0076\n",
      "Epoch 12, Batch 750/2400, Loss: 0.0057\n",
      "Epoch 12, Batch 800/2400, Loss: 0.0183\n",
      "Epoch 12, Batch 850/2400, Loss: 0.0063\n",
      "Epoch 12, Batch 900/2400, Loss: 0.0043\n",
      "Epoch 12, Batch 950/2400, Loss: 0.0050\n",
      "Epoch 12, Batch 1000/2400, Loss: 0.0036\n",
      "Epoch 12, Batch 1050/2400, Loss: 0.0136\n",
      "Epoch 12, Batch 1100/2400, Loss: 0.0063\n",
      "Epoch 12, Batch 1150/2400, Loss: 0.0053\n",
      "Epoch 12, Batch 1200/2400, Loss: 0.0068\n",
      "Epoch 12, Batch 1250/2400, Loss: 0.0064\n",
      "Epoch 12, Batch 1300/2400, Loss: 0.0058\n",
      "Epoch 12, Batch 1350/2400, Loss: 0.0089\n",
      "Epoch 12, Batch 1400/2400, Loss: 0.0056\n",
      "Epoch 12, Batch 1450/2400, Loss: 0.0102\n",
      "Epoch 12, Batch 1500/2400, Loss: 0.0076\n",
      "Epoch 12, Batch 1550/2400, Loss: 0.0065\n",
      "Epoch 12, Batch 1600/2400, Loss: 0.0057\n",
      "Epoch 12, Batch 1650/2400, Loss: 0.0048\n",
      "Epoch 12, Batch 1700/2400, Loss: 0.0035\n",
      "Epoch 12, Batch 1750/2400, Loss: 0.0067\n",
      "Epoch 12, Batch 1800/2400, Loss: 0.0100\n",
      "Epoch 12, Batch 1850/2400, Loss: 0.0065\n",
      "Epoch 12, Batch 1900/2400, Loss: 0.0085\n",
      "Epoch 12, Batch 1950/2400, Loss: 0.0064\n",
      "Epoch 12, Batch 2000/2400, Loss: 0.0089\n",
      "Epoch 12, Batch 2050/2400, Loss: 0.0046\n",
      "Epoch 12, Batch 2100/2400, Loss: 0.0046\n",
      "Epoch 12, Batch 2150/2400, Loss: 0.0100\n",
      "Epoch 12, Batch 2200/2400, Loss: 0.0087\n",
      "Epoch 12, Batch 2250/2400, Loss: 0.0049\n",
      "Epoch 12, Batch 2300/2400, Loss: 0.0103\n",
      "Epoch 12, Batch 2350/2400, Loss: 0.0040\n",
      "Epoch 12, Batch 2400/2400, Loss: 0.0043\n",
      "Epoch 12, Loss: 0.0069329259376293825\n",
      "Val Loss: 0.0080\n",
      "[INFO] Best model saved at epoch 12, val_loss=0.0080\n",
      "\n",
      "Epoch [13/20]\n",
      "Epoch 13, Batch 50/2400, Loss: 0.0096\n",
      "Epoch 13, Batch 100/2400, Loss: 0.0063\n",
      "Epoch 13, Batch 150/2400, Loss: 0.0086\n",
      "Epoch 13, Batch 200/2400, Loss: 0.0113\n",
      "Epoch 13, Batch 250/2400, Loss: 0.0082\n",
      "Epoch 13, Batch 300/2400, Loss: 0.0125\n",
      "Epoch 13, Batch 350/2400, Loss: 0.0040\n",
      "Epoch 13, Batch 400/2400, Loss: 0.0053\n",
      "Epoch 13, Batch 450/2400, Loss: 0.0103\n",
      "Epoch 13, Batch 500/2400, Loss: 0.0037\n",
      "Epoch 13, Batch 550/2400, Loss: 0.0057\n",
      "Epoch 13, Batch 600/2400, Loss: 0.0045\n",
      "Epoch 13, Batch 650/2400, Loss: 0.0074\n",
      "Epoch 13, Batch 700/2400, Loss: 0.0073\n",
      "Epoch 13, Batch 750/2400, Loss: 0.0051\n",
      "Epoch 13, Batch 800/2400, Loss: 0.0037\n",
      "Epoch 13, Batch 850/2400, Loss: 0.0049\n",
      "Epoch 13, Batch 900/2400, Loss: 0.0096\n",
      "Epoch 13, Batch 950/2400, Loss: 0.0039\n",
      "Epoch 13, Batch 1000/2400, Loss: 0.0076\n",
      "Epoch 13, Batch 1050/2400, Loss: 0.0136\n",
      "Epoch 13, Batch 1100/2400, Loss: 0.0039\n",
      "Epoch 13, Batch 1150/2400, Loss: 0.0045\n",
      "Epoch 13, Batch 1200/2400, Loss: 0.0112\n",
      "Epoch 13, Batch 1250/2400, Loss: 0.0060\n",
      "Epoch 13, Batch 1300/2400, Loss: 0.0070\n",
      "Epoch 13, Batch 1350/2400, Loss: 0.0048\n",
      "Epoch 13, Batch 1400/2400, Loss: 0.0025\n",
      "Epoch 13, Batch 1450/2400, Loss: 0.0108\n",
      "Epoch 13, Batch 1500/2400, Loss: 0.0036\n",
      "Epoch 13, Batch 1550/2400, Loss: 0.0081\n",
      "Epoch 13, Batch 1600/2400, Loss: 0.0050\n",
      "Epoch 13, Batch 1650/2400, Loss: 0.0077\n",
      "Epoch 13, Batch 1700/2400, Loss: 0.0037\n",
      "Epoch 13, Batch 1750/2400, Loss: 0.0093\n",
      "Epoch 13, Batch 1800/2400, Loss: 0.0053\n",
      "Epoch 13, Batch 1850/2400, Loss: 0.0041\n",
      "Epoch 13, Batch 1900/2400, Loss: 0.0069\n",
      "Epoch 13, Batch 1950/2400, Loss: 0.0075\n",
      "Epoch 13, Batch 2000/2400, Loss: 0.0070\n",
      "Epoch 13, Batch 2050/2400, Loss: 0.0065\n",
      "Epoch 13, Batch 2100/2400, Loss: 0.0121\n",
      "Epoch 13, Batch 2150/2400, Loss: 0.0052\n",
      "Epoch 13, Batch 2200/2400, Loss: 0.0038\n",
      "Epoch 13, Batch 2250/2400, Loss: 0.0059\n",
      "Epoch 13, Batch 2300/2400, Loss: 0.0078\n",
      "Epoch 13, Batch 2350/2400, Loss: 0.0063\n",
      "Epoch 13, Batch 2400/2400, Loss: 0.0118\n",
      "Epoch 13, Loss: 0.006721330793043307\n",
      "Val Loss: 0.0084\n",
      "\n",
      "Epoch [14/20]\n",
      "Epoch 14, Batch 50/2400, Loss: 0.0065\n",
      "Epoch 14, Batch 100/2400, Loss: 0.0043\n",
      "Epoch 14, Batch 150/2400, Loss: 0.0107\n",
      "Epoch 14, Batch 200/2400, Loss: 0.0115\n",
      "Epoch 14, Batch 250/2400, Loss: 0.0010\n",
      "Epoch 14, Batch 300/2400, Loss: 0.0065\n",
      "Epoch 14, Batch 350/2400, Loss: 0.0093\n",
      "Epoch 14, Batch 400/2400, Loss: 0.0046\n",
      "Epoch 14, Batch 450/2400, Loss: 0.0071\n",
      "Epoch 14, Batch 500/2400, Loss: 0.0038\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./checkpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir)\u001b[0m\n\u001b[0;32m    230\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tx_signal)\n\u001b[0;32m    231\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 232\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\adam.py:307\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, dataloader_train,dataloader_val, criterion, optimizer,scheduler, epochs, device, checkpoint_dir='./checkpoints')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
