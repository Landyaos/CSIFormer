{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import hdf5storage\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### 数据集预处理\n",
    "\n",
    "class MIMODataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csi_ls, csi_pre, csi_label):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param csi_ls: 导频CSI矩阵  [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :param csi: CSI矩阵 [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :param csi_pre: 历史CSI矩阵 [data_size, n_frame, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        \"\"\"\n",
    "        self.csi_ls = csi_ls\n",
    "        self.csi_pre = csi_pre\n",
    "        self.csi_label = csi_label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小\"\"\"\n",
    "        return self.csi_label.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        返回单个样本\n",
    "        :param idx: 样本索引\n",
    "        :return: 发射导频、接收导频、CSI矩阵\n",
    "        \"\"\"\n",
    "        return self.csi_ls[idx], self.csi_pre[idx], self.csi_label[idx]\n",
    "\n",
    "def dataset_preprocess(data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    csi_ls = torch.tensor(data['csiLSData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    csi_pre = torch.tensor(data['csiPreData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    csi_label = torch.tensor(data['csiLabelData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return MIMODataset(csi_ls, csi_pre, csi_label)\n",
    "\n",
    "\n",
    "# 残差块定义\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, in_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.activation(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return self.activation(x + residual)\n",
    "\n",
    "# 深度残差网络模型\n",
    "class DNNResCELS(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, num_blocks=4, n_subc=224 ,n_sym=14 ,n_tx=2, n_rx=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_subc = n_subc\n",
    "        self.n_sym = n_sym\n",
    "        self.n_tx = n_tx\n",
    "        self.n_rx = n_rx\n",
    "\n",
    "        # 输入层\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(n_subc * n_sym, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 残差块堆叠\n",
    "        self.res_blocks = nn.Sequential(*[\n",
    "            ResidualBlock(hidden_dim, hidden_dim*2)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_dim, n_subc * n_sym)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.reshape(-1 ,self.n_subc * self.n_sym, self.n_tx * self.n_rx * 2)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = x.reshape(-1, self.n_subc, self.n_sym, self.n_tx, self.n_rx, 2)\n",
    "        return x\n",
    "\n",
    "class ComplexMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param alpha: 第一部分损失的权重\n",
    "        :param beta:  第二部分损失的权重\n",
    "        \"\"\"\n",
    "        super(ComplexMSELoss, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        复数信道估计的均方误差 (MSE) 损失函数。\n",
    "        x_py: (batch_size, csi_matrix, 2)，估计值\n",
    "        y_py: (batch_size, csi_matrix, 2)，真实值\n",
    "        \"\"\"\n",
    "        diff = output - target  # 差值，形状保持一致\n",
    "        loss = torch.mean(diff[..., 0]**2 + diff[..., 1]**2)  # 实部和虚部平方和\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir='./checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    model.to(device)\n",
    "    # 查看是否有可用的最近 checkpoint\n",
    "    latest_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_pro_latest.pth')\n",
    "    best_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_pro_best.pth')\n",
    "\n",
    "    if os.path.isfile(latest_path):\n",
    "        print(f\"[INFO] Resuming training from '{latest_path}'\")\n",
    "        checkpoint = torch.load(latest_path, map_location=device)\n",
    "\n",
    "        # 加载模型、优化器、调度器状态\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', best_loss)\n",
    "        print(f\"[INFO] Resumed epoch {start_epoch}, best_loss={best_loss:.6f}\")\n",
    "    \n",
    "    # 分epoch训练\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{epochs}]\")\n",
    "        # --------------------- Train ---------------------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (csi_ls,csi_pre,csi_label) in enumerate(dataloader_train):\n",
    "            csi_ls = csi_ls.to(device)\n",
    "            csi_pre = csi_pre.to(device)\n",
    "            csi_label = csi_label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(csi_ls)\n",
    "            loss = criterion(output, csi_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader_train)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = total_loss / len(dataloader_train)\n",
    "        # 学习率调度器步进（根据策略）\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(train_loss)  # 对于 ReduceLROnPlateau 等需要传入指标的调度器\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader_train)}\")\n",
    "\n",
    "        # --------------------- Validate ---------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (csi_ls,csi_pre,csi_label) in enumerate(dataloader_val):\n",
    "                csi_ls = csi_ls.to(device)\n",
    "                csi_pre = csi_pre.to(device)\n",
    "                csi_label = csi_label.to(device)\n",
    "                output = model(csi_ls)\n",
    "                loss = criterion(output, csi_label)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(dataloader_val)\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # --------------------- Checkpoint 保存 ---------------------\n",
    "        # 1) 保存最新checkpoint（确保断点续训）\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "            'best_loss': best_loss,\n",
    "        }, latest_path)\n",
    "\n",
    "        # 2) 如果当前验证集 Loss 最佳，则保存为 best.pth\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, best_path)\n",
    "            print(f\"[INFO] Best model saved at epoch {epoch + 1}, val_loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "load done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"load data\")\n",
    "data_train = hdf5storage.loadmat('./data/raw/eqTrainData.mat')\n",
    "data_val = hdf5storage.loadmat('./data/raw/eqValData.mat')\n",
    "print(\"load done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_preprocess(data_train)\n",
    "dataset_val = dataset_preprocess(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Total trainable parameters: 7415360\n",
      "train model\n"
     ]
    }
   ],
   "source": [
    "# 主函数执行\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "lr = 1e-3\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "shuffle_flag = True\n",
    "model = DNNResCELS()\n",
    "criterion = ComplexMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "dataloader_val = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1)\n",
    "# 计算参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {count_parameters(model)}\")\n",
    "print('train model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20]\n",
      "Epoch 1, Batch 50/125, Loss: 0.0091\n",
      "Epoch 1, Batch 100/125, Loss: 0.0039\n",
      "Epoch 1, Loss: 0.03156015559099615\n",
      "Val Loss: 0.0032\n",
      "[INFO] Best model saved at epoch 1, val_loss=0.0032\n",
      "\n",
      "Epoch [2/20]\n",
      "Epoch 2, Batch 50/125, Loss: 0.0017\n",
      "Epoch 2, Batch 100/125, Loss: 0.0013\n",
      "Epoch 2, Loss: 0.0017294273860752583\n",
      "Val Loss: 0.0012\n",
      "[INFO] Best model saved at epoch 2, val_loss=0.0012\n",
      "\n",
      "Epoch [3/20]\n",
      "Epoch 3, Batch 50/125, Loss: 0.0008\n",
      "Epoch 3, Batch 100/125, Loss: 0.0006\n",
      "Epoch 3, Loss: 0.0008042760058306158\n",
      "Val Loss: 0.0011\n",
      "[INFO] Best model saved at epoch 3, val_loss=0.0011\n",
      "\n",
      "Epoch [4/20]\n",
      "Epoch 4, Batch 50/125, Loss: 0.0009\n",
      "Epoch 4, Batch 100/125, Loss: 0.0005\n",
      "Epoch 4, Loss: 0.0005949193753767758\n",
      "Val Loss: 0.0009\n",
      "[INFO] Best model saved at epoch 4, val_loss=0.0009\n",
      "\n",
      "Epoch [5/20]\n",
      "Epoch 5, Batch 50/125, Loss: 0.0006\n",
      "Epoch 5, Batch 100/125, Loss: 0.0005\n",
      "Epoch 5, Loss: 0.0004797114583197981\n",
      "Val Loss: 0.0008\n",
      "[INFO] Best model saved at epoch 5, val_loss=0.0008\n",
      "\n",
      "Epoch [6/20]\n",
      "Epoch 6, Batch 50/125, Loss: 0.0004\n",
      "Epoch 6, Batch 100/125, Loss: 0.0005\n",
      "Epoch 6, Loss: 0.00042820214689709245\n",
      "Val Loss: 0.0007\n",
      "[INFO] Best model saved at epoch 6, val_loss=0.0007\n",
      "\n",
      "Epoch [7/20]\n",
      "Epoch 7, Batch 50/125, Loss: 0.0006\n",
      "Epoch 7, Batch 100/125, Loss: 0.0004\n",
      "Epoch 7, Loss: 0.000406896916218102\n",
      "Val Loss: 0.0008\n",
      "\n",
      "Epoch [8/20]\n",
      "Epoch 8, Batch 50/125, Loss: 0.0003\n",
      "Epoch 8, Batch 100/125, Loss: 0.0003\n",
      "Epoch 8, Loss: 0.00038057028432376684\n",
      "Val Loss: 0.0007\n",
      "[INFO] Best model saved at epoch 8, val_loss=0.0007\n",
      "\n",
      "Epoch [9/20]\n",
      "Epoch 9, Batch 50/125, Loss: 0.0004\n",
      "Epoch 9, Batch 100/125, Loss: 0.0003\n",
      "Epoch 9, Loss: 0.00039800883224233986\n",
      "Val Loss: 0.0009\n",
      "\n",
      "Epoch [10/20]\n",
      "Epoch 10, Batch 50/125, Loss: 0.0003\n",
      "Epoch 10, Batch 100/125, Loss: 0.0003\n",
      "Epoch 10, Loss: 0.00031171533069573344\n",
      "Val Loss: 0.0007\n",
      "\n",
      "Epoch [11/20]\n",
      "Epoch 11, Batch 50/125, Loss: 0.0002\n",
      "Epoch 11, Batch 100/125, Loss: 0.0003\n",
      "Epoch 11, Loss: 0.00026133713533636185\n",
      "Val Loss: 0.0007\n",
      "[INFO] Best model saved at epoch 11, val_loss=0.0007\n",
      "\n",
      "Epoch [12/20]\n",
      "Epoch 12, Batch 50/125, Loss: 0.0003\n",
      "Epoch 12, Batch 100/125, Loss: 0.0003\n",
      "Epoch 12, Loss: 0.0002770815211115405\n",
      "Val Loss: 0.0007\n",
      "\n",
      "Epoch [13/20]\n",
      "Epoch 13, Batch 50/125, Loss: 0.0003\n",
      "Epoch 13, Batch 100/125, Loss: 0.0003\n",
      "Epoch 13, Loss: 0.0002737538332585245\n",
      "Val Loss: 0.0007\n",
      "\n",
      "Epoch [14/20]\n",
      "Epoch 14, Batch 50/125, Loss: 0.0002\n",
      "Epoch 14, Batch 100/125, Loss: 0.0003\n",
      "Epoch 14, Loss: 0.00026740095391869544\n",
      "Val Loss: 0.0006\n",
      "[INFO] Best model saved at epoch 14, val_loss=0.0006\n",
      "\n",
      "Epoch [15/20]\n",
      "Epoch 15, Batch 50/125, Loss: 0.0002\n",
      "Epoch 15, Batch 100/125, Loss: 0.0003\n",
      "Epoch 15, Loss: 0.00023301870841532945\n",
      "Val Loss: 0.0006\n",
      "[INFO] Best model saved at epoch 15, val_loss=0.0006\n",
      "\n",
      "Epoch [16/20]\n",
      "Epoch 16, Batch 50/125, Loss: 0.0002\n",
      "Epoch 16, Batch 100/125, Loss: 0.0003\n",
      "Epoch 16, Loss: 0.0002627335808938369\n",
      "Val Loss: 0.0007\n",
      "\n",
      "Epoch [17/20]\n",
      "Epoch 17, Batch 50/125, Loss: 0.0003\n",
      "Epoch 17, Batch 100/125, Loss: 0.0004\n",
      "Epoch 17, Loss: 0.00023496936273295432\n",
      "Val Loss: 0.0007\n",
      "\n",
      "Epoch [18/20]\n",
      "Epoch 18, Batch 50/125, Loss: 0.0002\n",
      "Epoch 18, Batch 100/125, Loss: 0.0002\n",
      "Epoch 18, Loss: 0.0002115161221008748\n",
      "Val Loss: 0.0007\n",
      "\n",
      "Epoch [19/20]\n",
      "Epoch 19, Batch 50/125, Loss: 0.0002\n",
      "Epoch 19, Batch 100/125, Loss: 0.0002\n",
      "Epoch 19, Loss: 0.0002170564626576379\n",
      "Val Loss: 0.0006\n",
      "[INFO] Best model saved at epoch 19, val_loss=0.0006\n",
      "\n",
      "Epoch [20/20]\n",
      "Epoch 20, Batch 50/125, Loss: 0.0003\n",
      "Epoch 20, Batch 100/125, Loss: 0.0002\n",
      "Epoch 20, Loss: 0.00021474842040333897\n",
      "Val Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "train_model(model, dataloader_train,dataloader_val, criterion, optimizer,scheduler, epochs, device, checkpoint_dir='./checkpoints')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
