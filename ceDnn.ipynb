{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import hdf5storage\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理与数据集构建\n",
    "class MIMODataset(Dataset):\n",
    "    def __init__(self, tx_pilot_signal, rx_pilot_signal, csi):\n",
    "        \"\"\"\n",
    "        输入数据说明：\n",
    "        tx_signal: [data_size, n_subc, n_sym, n_tx, 2] (实部虚部分量)\n",
    "        rx_signal: [data_size, n_subc, n_sym, n_rx, 2]\n",
    "        csi:       [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        \"\"\"\n",
    "        # 合并所有数据样本\n",
    "        self.data_size = tx_pilot_signal.shape[0]\n",
    "        self.tx_pilot_signal = tx_pilot_signal\n",
    "        self.rx_pilot_signal = rx_pilot_signal\n",
    "        self.csi = csi\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tx_pilot_signal[idx], self.rx_pilot_signal[idx], self.csi[idx] \n",
    "\n",
    "# 残差块定义\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, in_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.activation(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return self.activation(x + residual)\n",
    "\n",
    "# 深度残差网络模型\n",
    "class DNNResCE(nn.Module):\n",
    "    def __init__(self, input_dim=8, output_dim=8, hidden_dim=128, num_blocks=4, n_tx=2, n_rx=2):\n",
    "        super().__init__()\n",
    "        # 输入层\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 残差块堆叠\n",
    "        self.res_blocks = nn.Sequential(*[\n",
    "            ResidualBlock(hidden_dim, hidden_dim*2)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.n_tx = n_tx\n",
    "        self.n_rx = n_rx\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, tx_pilot_signal, rx_pilot_signal):\n",
    "        tx_pilot_signal = tx_pilot_signal.reshape(*tx_pilot_signal.shape[:3], -1)  # [batch_size, n_subc, n_sym, n_tx*2]\n",
    "        rx_pilot_signal = rx_pilot_signal.reshape(*rx_pilot_signal.shape[:3], -1) #[batch_size, n_subc, n_sym, n_rx*2]\n",
    "        x = torch.cat([tx_pilot_signal, rx_pilot_signal], dim=-1) # [batch_size, n_subc, n_sym, (n_tx + n_rx)*2]\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = x.reshape(*x.shape[:3],self.n_tx, self.n_rx,2)\n",
    "        return x\n",
    "\n",
    "\n",
    "def dataset_preprocess(data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    tx_pilot_signal = torch.tensor(data['txPilotData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    rx_pilot_signal = torch.tensor(data['rxPilotData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    csi = torch.tensor(data['csiLabelData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return MIMODataset(tx_pilot_signal, rx_pilot_signal, csi)\n",
    "\n",
    "class ComplexMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param alpha: 第一部分损失的权重\n",
    "        :param beta:  第二部分损失的权重\n",
    "        \"\"\"\n",
    "        super(ComplexMSELoss, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        复数信道估计的均方误差 (MSE) 损失函数。\n",
    "        x_py: (batch_size, csi_matrix, 2)，估计值\n",
    "        y_py: (batch_size, csi_matrix, 2)，真实值\n",
    "        \"\"\"\n",
    "        diff = output - target  # 差值，形状保持一致\n",
    "        loss = torch.mean(diff[..., 0]**2 + diff[..., 1]**2)  # 实部和虚部平方和\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir='./checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    model.to(device)\n",
    "    # 查看是否有可用的最近 checkpoint\n",
    "    latest_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_pro_latest.pth')\n",
    "    best_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_pro_best.pth')\n",
    "\n",
    "    if os.path.isfile(latest_path):\n",
    "        print(f\"[INFO] Resuming training from '{latest_path}'\")\n",
    "        checkpoint = torch.load(latest_path, map_location=device)\n",
    "\n",
    "        # 加载模型、优化器、调度器状态\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', best_loss)\n",
    "        print(f\"[INFO] Resumed epoch {start_epoch}, best_loss={best_loss:.6f}\")\n",
    "    \n",
    "    # 分epoch训练\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{epochs}]\")\n",
    "        # --------------------- Train ---------------------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (tx_pilot_signal,rx_pilot_signal,csi) in enumerate(dataloader_train):\n",
    "            tx_pilot_signal = tx_pilot_signal.to(device)\n",
    "            rx_pilot_signal = rx_pilot_signal.to(device)\n",
    "            csi = csi.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(tx_pilot_signal, rx_pilot_signal)\n",
    "            loss = criterion(output, csi)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader_train)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = total_loss / len(dataloader_train)\n",
    "        # 学习率调度器步进（根据策略）\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(train_loss)  # 对于 ReduceLROnPlateau 等需要传入指标的调度器\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader_train)}\")\n",
    "\n",
    "        # --------------------- Validate ---------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (tx_pilot_signal,rx_pilot_signal,csi) in enumerate(dataloader_val):\n",
    "                tx_pilot_signal = tx_pilot_signal.to(device)\n",
    "                rx_pilot_signal = rx_pilot_signal.to(device)\n",
    "                csi = csi.to(device)\n",
    "                output = model(tx_pilot_signal, rx_pilot_signal)\n",
    "                loss = criterion(output, csi)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(dataloader_val)\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # --------------------- Checkpoint 保存 ---------------------\n",
    "        # 1) 保存最新checkpoint（确保断点续训）\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "            'best_loss': best_loss,\n",
    "        }, latest_path)\n",
    "\n",
    "        # 2) 如果当前验证集 Loss 最佳，则保存为 best.pth\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, best_path)\n",
    "            print(f\"[INFO] Best model saved at epoch {epoch + 1}, val_loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "load done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"load data\")\n",
    "data_train = hdf5storage.loadmat('./data/raw/ceTrainData.mat')\n",
    "data_val = hdf5storage.loadmat('./data/raw/ceValData.mat')\n",
    "print(\"load done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 52, 14, 2, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data_train['txPilotData']\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Total trainable parameters: 265864\n",
      "train model\n",
      "\n",
      "Epoch [1/20]\n",
      "Epoch 1, Batch 50/63, Loss: 0.4817\n",
      "Epoch 1, Loss: 0.4597802980551644\n",
      "Val Loss: 0.4495\n",
      "[INFO] Best model saved at epoch 1, val_loss=0.4495\n",
      "\n",
      "Epoch [2/20]\n",
      "Epoch 2, Batch 50/63, Loss: 0.4842\n",
      "Epoch 2, Loss: 0.4521163333029974\n",
      "Val Loss: 0.4490\n",
      "[INFO] Best model saved at epoch 2, val_loss=0.4490\n",
      "\n",
      "Epoch [3/20]\n",
      "Epoch 3, Batch 50/63, Loss: 0.4814\n",
      "Epoch 3, Loss: 0.45024934363743613\n",
      "Val Loss: 0.4495\n",
      "\n",
      "Epoch [4/20]\n",
      "Epoch 4, Batch 50/63, Loss: 0.4355\n",
      "Epoch 4, Loss: 0.44986250807368566\n",
      "Val Loss: 0.4491\n",
      "\n",
      "Epoch [5/20]\n",
      "Epoch 5, Batch 50/63, Loss: 0.4362\n",
      "Epoch 5, Loss: 0.44961257586403497\n",
      "Val Loss: 0.4501\n",
      "\n",
      "Epoch [6/20]\n",
      "Epoch 6, Batch 50/63, Loss: 0.4424\n",
      "Epoch 6, Loss: 0.4492523618160732\n",
      "Val Loss: 0.4493\n",
      "\n",
      "Epoch [7/20]\n",
      "Epoch 7, Batch 50/63, Loss: 0.4410\n",
      "Epoch 7, Loss: 0.4483341480058337\n",
      "Val Loss: 0.4522\n",
      "\n",
      "Epoch [8/20]\n",
      "Epoch 8, Batch 50/63, Loss: 0.4590\n",
      "Epoch 8, Loss: 0.44784841934839886\n",
      "Val Loss: 0.4499\n",
      "\n",
      "Epoch [9/20]\n",
      "Epoch 9, Batch 50/63, Loss: 0.4298\n",
      "Epoch 9, Loss: 0.4478886832320501\n",
      "Val Loss: 0.4516\n",
      "\n",
      "Epoch [10/20]\n",
      "Epoch 10, Batch 50/63, Loss: 0.4504\n",
      "Epoch 10, Loss: 0.447378095653322\n",
      "Val Loss: 0.4521\n",
      "\n",
      "Epoch [11/20]\n",
      "Epoch 11, Batch 50/63, Loss: 0.4579\n",
      "Epoch 11, Loss: 0.4466748634974162\n",
      "Val Loss: 0.4524\n",
      "\n",
      "Epoch [12/20]\n",
      "Epoch 12, Batch 50/63, Loss: 0.4516\n",
      "Epoch 12, Loss: 0.44644691168315825\n",
      "Val Loss: 0.4521\n",
      "\n",
      "Epoch [13/20]\n",
      "Epoch 13, Batch 50/63, Loss: 0.4563\n",
      "Epoch 13, Loss: 0.4461003631826431\n",
      "Val Loss: 0.4531\n",
      "\n",
      "Epoch [14/20]\n",
      "Epoch 14, Batch 50/63, Loss: 0.4533\n",
      "Epoch 14, Loss: 0.4455897789152842\n",
      "Val Loss: 0.4513\n",
      "\n",
      "Epoch [15/20]\n",
      "Epoch 15, Batch 50/63, Loss: 0.4366\n",
      "Epoch 15, Loss: 0.4453539290125408\n",
      "Val Loss: 0.4529\n",
      "\n",
      "Epoch [16/20]\n",
      "Epoch 16, Batch 50/63, Loss: 0.4459\n",
      "Epoch 16, Loss: 0.44517799455022056\n",
      "Val Loss: 0.4547\n",
      "\n",
      "Epoch [17/20]\n",
      "Epoch 17, Batch 50/63, Loss: 0.4402\n",
      "Epoch 17, Loss: 0.44467836192675997\n",
      "Val Loss: 0.4534\n",
      "\n",
      "Epoch [18/20]\n",
      "Epoch 18, Batch 50/63, Loss: 0.4367\n",
      "Epoch 18, Loss: 0.44424457256756134\n",
      "Val Loss: 0.4536\n",
      "\n",
      "Epoch [19/20]\n",
      "Epoch 19, Batch 50/63, Loss: 0.4363\n",
      "Epoch 19, Loss: 0.4441866661821093\n",
      "Val Loss: 0.4522\n",
      "\n",
      "Epoch [20/20]\n",
      "Epoch 20, Batch 50/63, Loss: 0.4375\n",
      "Epoch 20, Loss: 0.4441897982642764\n",
      "Val Loss: 0.4547\n"
     ]
    }
   ],
   "source": [
    "# 主函数执行\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "lr = 1e-3\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "shuffle_flag = True\n",
    "model = DNNResCE()\n",
    "dataset_train = dataset_preprocess(data_train)\n",
    "dataset_val = dataset_preprocess(data_val)\n",
    "criterion = ComplexMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "dataloader_val = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1)\n",
    "# 计算参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {count_parameters(model)}\")\n",
    "print('train model')\n",
    "\n",
    "train_model(model, dataloader_train,dataloader_val, criterion, optimizer,scheduler, epochs, device, checkpoint_dir='./checkpoints')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
