{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import hdf5storage\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理与数据集构建\n",
    "class MIMODataset(Dataset):\n",
    "    def __init__(self, tx_signal, rx_signal, csi):\n",
    "        \"\"\"\n",
    "        输入数据说明：\n",
    "        tx_signal: [data_size, n_subc, n_sym, n_tx, 2] (实部虚部分量)\n",
    "        rx_signal: [data_size, n_subc, n_sym, n_rx, 2]\n",
    "        csi:       [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        \"\"\"\n",
    "        # 合并所有数据样本\n",
    "        self.data_size = tx_signal.shape[0]\n",
    "        self.tx_signal = tx_signal\n",
    "        self.rx_signal = rx_signal\n",
    "        self.csi = csi\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.csi[idx], self.rx_signal[idx], self.tx_signal[idx]\n",
    "\n",
    "# 残差块定义\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, in_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.activation(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return self.activation(x + residual)\n",
    "\n",
    "# 深度残差网络模型\n",
    "class DNNResEQ(nn.Module):\n",
    "    def __init__(self, input_dim=12, output_dim=4, hidden_dim=128, num_blocks=4):\n",
    "        super().__init__()\n",
    "        # 输入层\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 残差块堆叠\n",
    "        self.res_blocks = nn.Sequential(*[\n",
    "            ResidualBlock(hidden_dim, hidden_dim*2)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, csi, rx_signal):\n",
    "        csi = csi.reshape(*csi.shape[:3], -1)  # [batch_size, n_subc, n_sym, n_tx*n_rx*2]\n",
    "        rx_signal = rx_signal.reshape(*rx_signal.shape[:3], -1) #[batch_size, n_subc, n_sym, n_rx*2]\n",
    "        x = torch.cat([csi, rx_signal], dim=-1) # [batch_size, n_subc, n_sym, (n_tx*n_rx + n_rx)*2]\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = x.reshape(*x.shape[:3],2,2)\n",
    "        return x\n",
    "\n",
    "\n",
    "def dataset_preprocess(data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    tx_signal = torch.tensor(data['txSignalData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, 2]\n",
    "    rx_signal = torch.tensor(data['rxSignalData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_rx, 2]\n",
    "    csi = torch.tensor(data['csiLabelData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return MIMODataset(tx_signal, rx_signal, csi)\n",
    "\n",
    "class ComplexMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param alpha: 第一部分损失的权重\n",
    "        :param beta:  第二部分损失的权重\n",
    "        \"\"\"\n",
    "        super(ComplexMSELoss, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        复数信道估计的均方误差 (MSE) 损失函数。\n",
    "        x_py: (batch_size, csi_matrix, 2)，估计值\n",
    "        y_py: (batch_size, csi_matrix, 2)，真实值\n",
    "        \"\"\"\n",
    "        diff = output - target  # 差值，形状保持一致\n",
    "        loss = torch.mean(diff[..., 0]**2 + diff[..., 1]**2)  # 实部和虚部平方和\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir='./checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    model.to(device)\n",
    "    # 查看是否有可用的最近 checkpoint\n",
    "    latest_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_v1_latest.pth')\n",
    "    best_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_v1_best.pth')\n",
    "\n",
    "    if os.path.isfile(latest_path):\n",
    "        print(f\"[INFO] Resuming training from '{latest_path}'\")\n",
    "        checkpoint = torch.load(latest_path, map_location=device)\n",
    "\n",
    "        # 加载模型、优化器、调度器状态\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', best_loss)\n",
    "        print(f\"[INFO] Resumed epoch {start_epoch}, best_loss={best_loss:.6f}\")\n",
    "    \n",
    "    # 分epoch训练\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{epochs}]\")\n",
    "        # --------------------- Train ---------------------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (csi, rx_signal, tx_signal) in enumerate(dataloader_train):\n",
    "            csi = csi.to(device)\n",
    "            rx_signal = rx_signal.to(device)\n",
    "            tx_signal = tx_signal.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(csi, rx_signal)\n",
    "            loss = criterion(output, tx_signal)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader_train)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = total_loss / len(dataloader_train)\n",
    "        # 学习率调度器步进（根据策略）\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(train_loss)  # 对于 ReduceLROnPlateau 等需要传入指标的调度器\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader_train)}\")\n",
    "\n",
    "        # --------------------- Validate ---------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (csi, rx_signal, tx_signal) in enumerate(dataloader_val):\n",
    "                csi = csi.to(device)\n",
    "                rx_signal = rx_signal.to(device)\n",
    "                tx_signal = tx_signal.to(device)\n",
    "                output = model(csi, rx_signal)\n",
    "                loss = criterion(output, tx_signal)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(dataloader_val)\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # --------------------- Checkpoint 保存 ---------------------\n",
    "        # 1) 保存最新checkpoint（确保断点续训）\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "            'best_loss': best_loss,\n",
    "        }, latest_path)\n",
    "\n",
    "        # 2) 如果当前验证集 Loss 最佳，则保存为 best.pth\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, best_path)\n",
    "            print(f\"[INFO] Best model saved at epoch {epoch + 1}, val_loss={val_loss:.4f}\")\n",
    "        # 3) 每隔5个epoch保存当前epoch的权重\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, os.path.join(checkpoint_dir, model.__class__.__name__ + '_epoch_'+str(epoch)+'.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"load data\")\n",
    "data_train = hdf5storage.loadmat('./data/raw/eqTrainData.mat')\n",
    "data_val = hdf5storage.loadmat('./data/raw/eqValData.mat')\n",
    "print(\"load done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_preprocess(data_train)\n",
    "dataset_val = dataset_preprocess(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNResEQ()\n",
    "# 计算参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {count_parameters(model)}\")\n",
    "print('train model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数执行\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "lr = 1e-3\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "shuffle_flag = True\n",
    "criterion = ComplexMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "dataloader_val = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, dataloader_train,dataloader_val, criterion, optimizer,scheduler, epochs, device, checkpoint_dir='./checkpoints')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
