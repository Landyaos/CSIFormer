{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import hdf5storage\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gc\n",
    "\n",
    "\n",
    "# --- Dataset (Provided by User) ---\n",
    "class MIMODataset(Dataset):\n",
    "    \"\"\"\n",
    "    MIMO-OFDM 数据集加载器 (保持不变)\n",
    "    \"\"\"\n",
    "    def __init__(self, tx_signal, rx_signal, csi_ls, pilot_mask):\n",
    "        self.data_size = tx_signal.shape[0]\n",
    "        self.tx_signal = tx_signal\n",
    "        self.rx_signal = rx_signal\n",
    "        self.csi_ls = csi_ls\n",
    "        self.pilot_mask = pilot_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # CSI, TxPilots (Tx * Mask), Rx, Target Tx\n",
    "        return (\n",
    "            self.csi_ls[idx],                  # [S, F, Nt, Nr, 2]\n",
    "            self.tx_signal[idx] * self.pilot_mask, # [S, F, Nt, 2] - 已应用掩码\n",
    "            self.rx_signal[idx],               # [S, F, Nr, 2]\n",
    "            self.tx_signal[idx]                # [S, F, Nt, 2] - Target\n",
    "        )\n",
    "\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise Separable Convolution as used in the paper (implied) and Xception/MobileNet.\n",
    "    Uses depth_multiplier=1 as a base, similar to standard MobileNet blocks.\n",
    "    The paper mentions a multiplier of 2, which would mean doubling channels in depthwise.\n",
    "    Here we implement the more standard version first.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super().__init__()\n",
    "        # Note: Paper uses (3,3) filters mostly. Dilation is applied.\n",
    "        # Padding needs to be calculated to keep dimensions same: padding = (kernel_size - 1) * dilation // 2\n",
    "        effective_kernel_size = (kernel_size - 1) * dilation + 1\n",
    "        padding = effective_kernel_size // 2\n",
    "\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride,\n",
    "                                   padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                                   padding=0, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-activation ResNet Block based on Figure 3 and Table I.\n",
    "    Uses Depthwise Separable Convolutions.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1,1)):\n",
    "        super().__init__()\n",
    "        # Effective padding calculation for dilation\n",
    "        eff_k_h = (kernel_size - 1) * dilation[0] + 1\n",
    "        eff_k_w = (kernel_size - 1) * dilation[1] + 1\n",
    "        padding = (eff_k_h // 2, eff_k_w // 2)\n",
    "\n",
    "        # Separable Conv 1 (includes BN -> ReLU -> Depthwise -> Pointwise(1x1))\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        # Paper uses depth_multiplier=2? Let's stick to 1 for now, standard separable conv\n",
    "        # Depthwise part\n",
    "        self.depthwise1 = nn.Conv2d(channels, channels, kernel_size, stride=1,\n",
    "                                    padding=padding, dilation=dilation, groups=channels, bias=False)\n",
    "        # Pointwise part (1x1) - Projects back to 'channels' dimension\n",
    "        self.pointwise1 = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        # Separable Conv 2\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # Depthwise part\n",
    "        self.depthwise2 = nn.Conv2d(channels, channels, kernel_size, stride=1,\n",
    "                                    padding=padding, dilation=dilation, groups=channels, bias=False)\n",
    "        # Pointwise part (1x1)\n",
    "        self.pointwise2 = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.depthwise1(out)\n",
    "        out = self.pointwise1(out) # Completes the first separable conv block implied in Fig 3\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.depthwise2(out)\n",
    "        out = self.pointwise2(out) # Completes the second separable conv block\n",
    "\n",
    "        out += residual # Add residual connection\n",
    "        return out\n",
    "\n",
    "class DeepRx(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepRx adaptation for MIMO-OFDM Equalization.\n",
    "    Outputs estimated transmit symbols.\n",
    "    Input dimensions based on MIMODataset:\n",
    "    - csi_ls:    [B, S, F, Nt, Nr, 2]\n",
    "    - tx_pilots: [B, S, F, Nt, 2] (Tx Signal * Pilot Mask)\n",
    "    - rx_signal: [B, S, F, Nr, 2]\n",
    "    Output dimension:\n",
    "    - tx_est:    [B, S, F, Nt, 2]\n",
    "    Internal Tensor Format: [B, C, S, F]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_tx=2, n_rx=2, num_blocks=11, channels=[64, 128, 256]):\n",
    "        super().__init__()\n",
    "        self.n_tx = n_tx\n",
    "        self.n_rx = n_rx\n",
    "\n",
    "        # Calculate input channels after flattening antennas and real/imag\n",
    "        # Input: csi_ls (Nt*Nr*2), tx_pilots (Nt*2), rx_signal (Nr*2)\n",
    "        input_channels = (n_tx * n_rx * 2) + (n_tx * 2) + (n_rx * 2) # 8 + 4 + 4 = 16 for 2x2\n",
    "\n",
    "        # --- Network Layers based on Table I ---\n",
    "        # Initial Convolution (Conv. In)\n",
    "        self.conv_in = nn.Conv2d(input_channels, channels[0], kernel_size=3, stride=1, padding=1, bias=False) # 64 filters\n",
    "\n",
    "        # ResNet Blocks\n",
    "        # Dilation values from paper Table 1 for 11 blocks (adjust indices for 0-based)\n",
    "        # Blocks 0,1: (1,1) -> channels[0] (64)\n",
    "        # Blocks 2,3,4: (2,3) -> channels[1] (128)\n",
    "        # Block 5: (3,6) -> channels[2] (256)\n",
    "        # Block 6,7,8: (2,3) -> channels[1] (128)  <- paper goes back down here\n",
    "        # Blocks 9,10: (1,1) -> channels[0] (64)\n",
    "        # Note: Paper architecture goes 64->128->256->128->64. Let's follow that.\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential()\n",
    "        current_channels = channels[0]\n",
    "\n",
    "        # Block 0, 1 (dilation 1,1), channels[0]\n",
    "        self.resnet_blocks.add_module(\"res0\", ResNetBlock(current_channels, dilation=(1,1)))\n",
    "        self.resnet_blocks.add_module(\"res1\", ResNetBlock(current_channels, dilation=(1,1)))\n",
    "\n",
    "        # Block 2 (transition to channels[1])\n",
    "        self.resnet_blocks.add_module(\"proj2\", nn.Sequential(\n",
    "            nn.BatchNorm2d(current_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(current_channels, channels[1], kernel_size=1, stride=1, bias=False) # Projection\n",
    "        ))\n",
    "        current_channels = channels[1]\n",
    "        self.resnet_blocks.add_module(\"res2\", ResNetBlock(current_channels, dilation=(2,3)))\n",
    "\n",
    "        # Block 3, 4 (dilation 2,3), channels[1]\n",
    "        self.resnet_blocks.add_module(\"res3\", ResNetBlock(current_channels, dilation=(2,3)))\n",
    "        self.resnet_blocks.add_module(\"res4\", ResNetBlock(current_channels, dilation=(2,3)))\n",
    "\n",
    "        # Block 5 (transition to channels[2])\n",
    "        self.resnet_blocks.add_module(\"proj5\", nn.Sequential(\n",
    "             nn.BatchNorm2d(current_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(current_channels, channels[2], kernel_size=1, stride=1, bias=False) # Projection\n",
    "        ))\n",
    "        current_channels = channels[2]\n",
    "        self.resnet_blocks.add_module(\"res5\", ResNetBlock(current_channels, dilation=(3,6))) # Paper uses (3,6) here\n",
    "\n",
    "        # Block 6 (transition back to channels[1])\n",
    "        self.resnet_blocks.add_module(\"proj6\", nn.Sequential(\n",
    "             nn.BatchNorm2d(current_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(current_channels, channels[1], kernel_size=1, stride=1, bias=False) # Projection\n",
    "        ))\n",
    "        current_channels = channels[1]\n",
    "        self.resnet_blocks.add_module(\"res6\", ResNetBlock(current_channels, dilation=(2,3)))\n",
    "\n",
    "        # Block 7, 8 (dilation 2,3), channels[1]\n",
    "        self.resnet_blocks.add_module(\"res7\", ResNetBlock(current_channels, dilation=(2,3)))\n",
    "        self.resnet_blocks.add_module(\"res8\", ResNetBlock(current_channels, dilation=(2,3)))\n",
    "\n",
    "        # Block 9 (transition back to channels[0])\n",
    "        self.resnet_blocks.add_module(\"proj9\", nn.Sequential(\n",
    "             nn.BatchNorm2d(current_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(current_channels, channels[0], kernel_size=1, stride=1, bias=False) # Projection\n",
    "        ))\n",
    "        current_channels = channels[0]\n",
    "        self.resnet_blocks.add_module(\"res9\", ResNetBlock(current_channels, dilation=(1,1)))\n",
    "\n",
    "        # Block 10 (dilation 1,1), channels[0]\n",
    "        self.resnet_blocks.add_module(\"res10\", ResNetBlock(current_channels, dilation=(1,1)))\n",
    "\n",
    "        # Final Batch Norm and ReLU before output conv\n",
    "        self.bn_out = nn.BatchNorm2d(current_channels)\n",
    "        self.relu_out = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Output Convolution (Conv. Out)\n",
    "        # Output should be the estimated tx_signal: [B, S, F, Nt, 2]\n",
    "        # So, need Nt*2 output channels\n",
    "        output_channels = n_tx * 2\n",
    "        self.conv_out = nn.Conv2d(current_channels, output_channels, kernel_size=3, stride=1, padding=1, bias=True) # Use bias for output layer\n",
    "\n",
    "    def forward(self, csi_ls, tx_pilots, rx_signal):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Input shapes:\n",
    "        - csi_ls:    [B, S, F, Nt, Nr, 2]\n",
    "        - tx_pilots: [B, S, F, Nt, 2]\n",
    "        - rx_signal: [B, S, F, Nr, 2]\n",
    "        \"\"\"\n",
    "        B, S, F, _, _, _ = csi_ls.shape\n",
    "\n",
    "        # Reshape and Permute inputs to [B, C, S, F] format\n",
    "        csi_ls_r = csi_ls.permute(0, 3, 4, 5, 1, 2).reshape(B, self.n_tx * self.n_rx * 2, S, F)\n",
    "        tx_pilots_r = tx_pilots.permute(0, 3, 4, 1, 2).reshape(B, self.n_tx * 2, S, F)\n",
    "        rx_signal_r = rx_signal.permute(0, 3, 4, 1, 2).reshape(B, self.n_rx * 2, S, F)\n",
    "\n",
    "        # Concatenate along the channel dimension\n",
    "        x = torch.cat([csi_ls_r, tx_pilots_r, rx_signal_r], dim=1)\n",
    "\n",
    "        # Pass through the network\n",
    "        x = self.conv_in(x)\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = self.bn_out(x)\n",
    "        x = self.relu_out(x)\n",
    "        x = self.conv_out(x) # Output shape: [B, Nt*2, S, F]\n",
    "\n",
    "        # Reshape output to [B, S, F, Nt, 2]\n",
    "        output = x.reshape(B, self.n_tx, 2, S, F).permute(0, 3, 4, 1, 2)\n",
    "\n",
    "        return output\n",
    "\n",
    "def dataset_preprocess(data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    pilot_mask = torch.zeros((256, 14, 2 , 2), dtype=torch.float32)\n",
    "    indices_ant1 = torch.tensor([\n",
    "        17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61, 65, 69, 73, 77,\n",
    "        81, 85, 89, 93, 97, 101, 105, 109, 113, 117, 121, 125, 133, 137,\n",
    "        141, 145, 149, 153, 157, 161, 165, 169, 173, 177, 181, 185, 189,\n",
    "        193, 197, 201, 205, 209, 213, 217, 221, 225, 229, 233, 237, 241\n",
    "    ])-1\n",
    "\n",
    "    indices_ant2 = torch.tensor([\n",
    "        18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62, 66, 70, 74, 78,\n",
    "        82, 86, 90, 94, 98, 102, 106, 110, 114, 118, 122, 126, 130, 134,\n",
    "        138, 142, 146, 150, 154, 158, 162, 166, 170, 174, 178, 182, 186,\n",
    "        190, 194, 198, 202, 206, 210, 214, 218, 222, 226, 230, 234, 240\n",
    "    ])-1\n",
    "\n",
    "    indices = torch.tensor(\n",
    "        [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, \n",
    "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, \n",
    "        57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, \n",
    "        77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, \n",
    "        97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, \n",
    "        114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, \n",
    "        131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, \n",
    "        147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, \n",
    "        163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, \n",
    "        179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, \n",
    "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, \n",
    "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, \n",
    "        227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241]\n",
    "    )-1\n",
    "\n",
    "    pilot_mask[indices_ant1,:,0,:] = 1\n",
    "    pilot_mask[indices_ant2,:,1,:] = 1\n",
    "    pilot_mask = pilot_mask[indices]\n",
    "    print(pilot_mask.shape)\n",
    "\n",
    "    csi_ls = torch.tensor(data['csiLSData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    tx_signal = torch.tensor(data['txSignalData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, 2]\n",
    "    rx_signal = torch.tensor(data['rxSignalData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_rx, 2]\n",
    "    csi = torch.tensor(data['csiLabelData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    \n",
    "    del data\n",
    "    gc.collect()\n",
    "    return MIMODataset(tx_signal, rx_signal, csi_ls, pilot_mask)\n",
    "\n",
    "\n",
    "\n",
    "class ComplexMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param alpha: 第一部分损失的权重\n",
    "        :param beta:  第二部分损失的权重\n",
    "        \"\"\"\n",
    "        super(ComplexMSELoss, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, csi_est, csi_label):\n",
    "        \"\"\"\n",
    "        复数信道估计的均方误差 (MSE) 损失函数。\n",
    "        x_py: (batch_size, csi_matrix, 2)，估计值\n",
    "        y_py: (batch_size, csi_matrix, 2)，真实值\n",
    "        \"\"\"\n",
    "        diff = csi_est - csi_label  # 差值，形状保持一致\n",
    "        loss = torch.mean(torch.square(torch.sqrt(torch.square(diff[...,0]) + torch.square(diff[...,1]))))\n",
    "        return loss\n",
    "        \n",
    "# 计算参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    model.to(device)\n",
    "    # 查看是否有可用的最近 checkpoint\n",
    "    latest_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_v1_latest.pth')\n",
    "    best_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_v1_best.pth')\n",
    "\n",
    "    if os.path.isfile(latest_path):\n",
    "        print(f\"[INFO] Resuming training from '{latest_path}'\")\n",
    "        checkpoint = torch.load(latest_path, map_location=device)\n",
    "\n",
    "        # 加载模型、优化器、调度器状态\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', best_loss)\n",
    "        print(f\"[INFO] Resumed epoch {start_epoch}, best_loss={best_loss:.6f}\")\n",
    "    \n",
    "    # 分epoch训练\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{epochs}]\")\n",
    "        # --------------------- Train ---------------------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (csi, pilot, rx_signal, tx_signal) in enumerate(dataloader_train):\n",
    "            csi = csi.to(device)\n",
    "            pilot = pilot.to(device)\n",
    "            rx_signal = rx_signal.to(device)\n",
    "            tx_signal = tx_signal.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(csi, pilot, rx_signal)\n",
    "            loss = criterion(output, tx_signal)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Batch ={batch_idx}\")\n",
    "            if (batch_idx + 1) % 2 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader_train)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = total_loss / len(dataloader_train)\n",
    "        # 学习率调度器步进（根据策略）\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(train_loss)  # 对于 ReduceLROnPlateau 等需要传入指标的调度器\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader_train)}\")\n",
    "\n",
    "        # --------------------- Validate ---------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (csi, pilot, rx_signal, tx_signal) in enumerate(dataloader_val):\n",
    "                csi = csi.to(device)\n",
    "                pilot = pilot.to(device)\n",
    "                rx_signal = rx_signal.to(device)\n",
    "                tx_signal = tx_signal.to(device)\n",
    "                output = model(csi, pilot, rx_signal)\n",
    "                loss = criterion(output, tx_signal)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(dataloader_val)\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # --------------------- Checkpoint 保存 ---------------------\n",
    "        # 1) 保存最新checkpoint（确保断点续训）\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "            'best_loss': best_loss,\n",
    "        }, latest_path)\n",
    "\n",
    "        # 2) 如果当前验证集 Loss 最佳，则保存为 best.pth\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, best_path)\n",
    "            print(f\"[INFO] Best model saved at epoch {epoch + 1}, val_loss={val_loss:.4f}\")\n",
    "        # 3) 每隔5个epoch保存当前epoch的权重\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, os.path.join(checkpoint_dir, model.__class__.__name__ + '_epoch_'+str(epoch)+'.pth'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "load done\n"
     ]
    }
   ],
   "source": [
    "print(\"load data\")\n",
    "# data_train = hdf5storage.loadmat('/root/autodl-tmp/data/raw/trainData.mat')\n",
    "# data_val = hdf5storage.loadmat('/root/autodl-tmp/data/raw/valData.mat')\n",
    "# checkpoint_dir = '/root/autodl-tmp/checkpoints'\n",
    "checkpoint_dir = './checkpoints'\n",
    "data_train = hdf5storage.loadmat('F:/dataset/valDataFinal.mat')\n",
    "data_val = hdf5storage.loadmat('F:/dataset/valDataFinal.mat')\n",
    "print(\"load done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Total trainable parameters: 483332\n",
      "train model\n",
      "torch.Size([224, 14, 2, 2])\n",
      "torch.Size([224, 14, 2, 2])\n",
      "\n",
      "Epoch [1/1]\n",
      "Epoch 1, Batch 2/112, Loss: 0.8627\n",
      "Epoch 1, Batch 4/112, Loss: 0.6990\n",
      "Epoch 1, Batch 6/112, Loss: 0.6120\n",
      "Epoch 1, Batch 8/112, Loss: 0.5751\n",
      "Epoch 1, Batch 10/112, Loss: 0.5527\n",
      "Epoch 1, Batch 12/112, Loss: 0.5310\n",
      "Epoch 1, Batch 14/112, Loss: 0.5242\n",
      "Epoch 1, Batch 16/112, Loss: 0.5195\n",
      "Epoch 1, Batch 18/112, Loss: 0.5131\n",
      "Epoch 1, Batch 20/112, Loss: 0.5103\n",
      "Epoch 1, Batch 22/112, Loss: 0.5087\n",
      "Epoch 1, Batch 24/112, Loss: 0.5052\n",
      "Epoch 1, Batch 26/112, Loss: 0.5054\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m dataloader_val \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mdataset_val, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle_flag)\n\u001b[0;32m     17\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir)\u001b[0m\n\u001b[0;32m    344\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tx_signal)\n\u001b[0;32m    345\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 346\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    303\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 主函数执行\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "lr = 1e-3\n",
    "epochs = 1\n",
    "batch_size = 36\n",
    "shuffle_flag = True\n",
    "model = DeepRx()\n",
    "print(f\"Total trainable parameters: {count_parameters(model)}\")\n",
    "print('train model')\n",
    "dataset_train = dataset_preprocess(data_train)\n",
    "dataset_val = dataset_preprocess(data_val)\n",
    "criterion = ComplexMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "dataloader_val = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1)\n",
    "train_model(model, dataloader_train,dataloader_val, criterion, optimizer,scheduler, epochs, device, checkpoint_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
