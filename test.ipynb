{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import hdf5storage\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import hdf5storage\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gc\n",
    "\n",
    "\n",
    "# ##### 数据集预处理\n",
    "\n",
    "class CSIFormerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csi_ls, csi_pre, csi_label):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param csi_ls: 导频CSI矩阵  [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :param csi: CSI矩阵 [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :param csi_pre: 历史CSI矩阵 [data_size, n_frame, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        \"\"\"\n",
    "        self.csi_ls = csi_ls\n",
    "        self.csi_pre = csi_pre\n",
    "        self.csi_label = csi_label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小\"\"\"\n",
    "        return self.csi_label.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        返回单个样本\n",
    "        :param idx: 样本索引\n",
    "        :return: 发射导频、接收导频、CSI矩阵\n",
    "        \"\"\"\n",
    "        return self.csi_ls[idx], self.csi_pre[idx], self.csi_label[idx]\n",
    "\n",
    "def dataset_preprocess(data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    csi_ls = torch.tensor(data['csiLSData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    csi_pre = torch.tensor(data['csiPreData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    csi_label = torch.tensor(data['csiLabelData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return CSIFormerDataset(csi_ls, csi_pre, csi_label)\n",
    "\n",
    "###############################################################################\n",
    "# 正弦/余弦位置编码\n",
    "###############################################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        :param d_model: 嵌入特征的维度\n",
    "        :param max_len: 序列的最大长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建 [max_len, d_model] 的位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度\n",
    "        pe = pe.unsqueeze(0)  # 增加 batch 维度\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: 输入张量 [B, seq_len, d_model]\n",
    "        :return: 加入位置编码的张量 [B, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "###############################################################################\n",
    "# 第一部分：CSIFormer (编码器)\n",
    "###############################################################################\n",
    "class CSIEncoder(nn.Module):\n",
    "    def __init__(self, d_model=256, nhead=8, n_layers=6, n_tx=2, n_rx=2, max_len=5000):\n",
    "        \"\"\"\n",
    "        编码器模块\n",
    "        :param d_model: Transformer 嵌入维度\n",
    "        :param nhead: 多头注意力头数\n",
    "        :param n_layers: Transformer 层数\n",
    "        :param n_tx: 发射天线数\n",
    "        :param n_rx: 接收天线数\n",
    "        :param max_len: 序列的最大长度\n",
    "        \"\"\"\n",
    "        super(CSIEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_tx = n_tx\n",
    "        self.num_rx = n_rx\n",
    "\n",
    "        # 线性层将输入映射到 d_model 维度\n",
    "        self.input_proj = nn.Linear(n_tx * n_rx * 2, d_model)\n",
    "\n",
    "        # 位置编码\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Transformer 编码器\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=2048,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, csi_ls):\n",
    "        \"\"\"\n",
    "        :param csi_ls: 当前帧的导频估计 [B, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :return: 编码后的特征 [B, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        B, n_subc, n_sym, n_tx, n_rx, _ = csi_ls.shape\n",
    "\n",
    "        # 展平 CSI 矩阵并投影到 d_model\n",
    "        csi_ls = csi_ls.view(B, n_subc, n_sym, -1)\n",
    "        input_features = self.input_proj(csi_ls)  # [B, n_subc, n_sym, d_model]\n",
    "\n",
    "        # 展平 (n_subc, n_sym) 维度为 seq_len\n",
    "        input_features = input_features.view(B, n_subc * n_sym, self.d_model)\n",
    "\n",
    "        # 添加位置编码\n",
    "        input_features = self.pos_encoder(input_features)\n",
    "\n",
    "        # Transformer 编码器\n",
    "        output_features = self.transformer_encoder(input_features)\n",
    "        return output_features\n",
    "\n",
    "###############################################################################\n",
    "# 第二部分：EnhancedCSIDecoder (解码器)\n",
    "###############################################################################\n",
    "class EnhancedCSIDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, nhead=8, n_layers=6, n_tx=2, n_rx=2, max_len=5000):\n",
    "        \"\"\"\n",
    "        :param d_model: Decoder 嵌入维度\n",
    "        :param nhead: 注意力头数\n",
    "        :param n_layers: 解码器层数\n",
    "        :param n_tx: 发射天线数\n",
    "        :param n_rx: 接收天线数\n",
    "        :param max_len: 序列的最大长度\n",
    "        \"\"\"\n",
    "        super(EnhancedCSIDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_tx = n_tx\n",
    "        self.num_rx = n_rx\n",
    "\n",
    "        # Transformer 解码器 (batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead,\n",
    "                dim_feedforward=2048,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "\n",
    "        # 位置编码\n",
    "        self.pos_query = PositionalEncoding(d_model, max_len)\n",
    "        self.pos_memory = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # 输出映射层，将 d_model 映射回原始 CSI 空间\n",
    "        self.output_proj = nn.Linear(d_model, n_tx * n_rx * 2)\n",
    "\n",
    "        # 投影历史 CSI 到 d_model 维度\n",
    "        self.memory_proj = nn.Linear(n_tx * n_rx * 2, d_model)\n",
    "\n",
    "    def forward(self, encoder_features, previous_csi):\n",
    "        \"\"\"\n",
    "        :param encoder_features: 编码器的输出特征 [B, seq_len, d_model]\n",
    "        :param previous_csi:    前 n 帧 CSI    [B, n_frame, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :return: 增强后的当前帧 CSI [B, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        \"\"\"\n",
    "        B, seq_len, _ = encoder_features.shape\n",
    "        _, n_frame, n_subc, n_sym, n_tx, n_rx, _ = previous_csi.shape\n",
    "        # 添加 Query 的位置编码\n",
    "        query = self.pos_query(encoder_features)\n",
    "        # ============= 处理 Memory (previous_csi) =============\n",
    "        # 展平历史 CSI 为 [B, n_frames, n_subc, n_sym, n_tx * n_rx * 2]\n",
    "        memory = previous_csi.view(B, n_frame, n_subc, n_sym, -1)\n",
    "        # 投影到 d_model 维度\n",
    "        memory = self.memory_proj(memory)  # [B, n_frames, n_subc, n_sym, d_model]\n",
    "        # 展平历史序列为 [B, seq_len_m, d_model]\n",
    "        memory = memory.view(B, n_frame * n_subc * n_sym, self.d_model)\n",
    "        memory = self.pos_memory(memory)\n",
    "        \n",
    "        # ============= 解码器 =============\n",
    "        # 解码器输入 Query: [B, seq_len, d_model], Memory: [B, seq_len_m, d_model]\n",
    "        enhanced_features = self.transformer_decoder(tgt=query, memory=memory)  # [B, seq_len, d_model]\n",
    "\n",
    "        # 映射到 CSI 空间\n",
    "        enhanced_csi = self.output_proj(enhanced_features)  # [B, seq_len, n_tx * n_rx * 2]\n",
    "\n",
    "        # 恢复形状为 [B, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        enhanced_csi = enhanced_csi.view(B, n_subc, n_sym, n_tx, n_rx, 2)\n",
    "        return enhanced_csi\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# CSIFormer：同时包含 Encoder 和 Decoder，批维在前\n",
    "###############################################################################\n",
    "class CSIFormer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=256, \n",
    "                 nhead=2, \n",
    "                 n_layers=4, \n",
    "                 n_tx=2, \n",
    "                 n_rx=2):\n",
    "        \"\"\"\n",
    "        同时包含：\n",
    "        1) CSIEncoder (编码器): 根据导频估计当前帧\n",
    "        2) EnhancedCSIDecoder (解码器): 利用前 n 帧和当前帧初步估计进行增强\n",
    "        :param d_model, nhead, n_layers: Transformer相关超参\n",
    "        :param n_tx, n_rx: 发射/接收天线数\n",
    "        :param n_frame: 前 n 帧参考数\n",
    "        \"\"\"\n",
    "        super(CSIFormer, self).__init__()\n",
    "        self.encoder = CSIEncoder(d_model, nhead, n_layers, n_rx, n_rx)\n",
    "        self.decoder = EnhancedCSIDecoder(d_model, nhead, n_layers, n_tx, n_rx)\n",
    "\n",
    "\n",
    "    def forward(self, csi_ls, previous_csi):\n",
    "        \"\"\"\n",
    "        :param csi_ls: 当前帧的导频估计 [B, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :param previous_csi: 前 n 帧历史 CSI [B, n_frame, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :return: (csi_enc, csi_dec)\n",
    "            csi_enc: 初步估计 [B, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "            csi_dec: 增强估计 [B, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        \"\"\"\n",
    "        # (1) 编码器：利用导频生成当前帧的初步 CSI 特征\n",
    "        csi_enc = self.encoder(csi_ls)  # [B, seq_len, d_model]\n",
    "        # (2) 解码器：结合前 n 帧的 CSI 与 csi_enc，输出增强后的 CSI\n",
    "        csi_dec = self.decoder(csi_enc, previous_csi)  # [B, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        return csi_dec\n",
    "\n",
    "def load_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CSIFormer().to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join('./checkpoints', model.__class__.__name__ + '_pro_best.pth'), map_location=device)['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "def infer(model, csi_ls, pre_csi):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    csi_ls = torch.unsqueeze(torch.tensor(csi_ls, dtype=torch.float32).to(device),0).contiguous()\n",
    "    pre_csi = torch.unsqueeze(torch.tensor(pre_csi, dtype=torch.float32).to(device),0).contiguous()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        csi_est = model(csi_ls ,pre_csi)\n",
    "    return torch.squeeze(csi_est).cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = hdf5storage.loadmat('./data/raw/valData.mat')\n",
    "csi_ls = torch.tensor(data['csiLSData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "csi_pre = torch.tensor(data['csiPreData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "csi_label = torch.tensor(data['csiLabelData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "c = nn.MSELoss()\n",
    "index = 0\n",
    "model = load_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "csi_ls_i = torch.unsqueeze(csi_ls[index].to(device),0).contiguous()\n",
    "pre_csi_i = torch.unsqueeze(csi_pre[index].to(device),0).contiguous()\n",
    "csi_label_i = torch.unsqueeze(csi_label[index].to(device),0).contiguous()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    csi_est = model(csi_ls_i ,pre_csi_i)\n",
    "c(csi_est, csi_label_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from custom function: 3.341095209121704\n",
      "Equivalent MATLAB Loss: 3.341095209121704\n",
      "Equivalent MATLAB Loss: 1.6705477237701416\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def complex_mse_loss(x_py, y_py):\n",
    "    \"\"\"\n",
    "    计算 x_py 和 y_py 的均方误差，用于复数输入。\n",
    "    x_py: (batch_size, 4, 4, 2) -> 最后一个维度是实部和虚部\n",
    "    y_py: (batch_size, 4, 4, 2) -> 最后一个维度是实部和虚部\n",
    "    \"\"\"\n",
    "    # 计算实部和虚部的差\n",
    "    diff = x_py - y_py  # (batch_size, 4, 4, 2)\n",
    "    # 计算实部和虚部差的平方和\n",
    "    mse = torch.mean(diff[..., 0]**2 + diff[..., 1]**2)  # 取最后一个维度的平方和\n",
    "    return mse\n",
    "# 生成随机复数矩阵并分解\n",
    "x = torch.randn(4, 4) + 1j * torch.randn(4, 4)\n",
    "y = torch.randn(4, 4) + 1j * torch.randn(4, 4)\n",
    "\n",
    "# 分解为实部和虚部\n",
    "x_py = torch.stack([x.real, x.imag], dim=-1)  # (4, 4, 2)\n",
    "y_py = torch.stack([y.real, y.imag], dim=-1)  # (4, 4, 2)\n",
    "\n",
    "# 使用自定义损失函数\n",
    "loss_fn = complex_mse_loss\n",
    "loss = loss_fn(x_py, y_py)\n",
    "c = nn.MSELoss()\n",
    "c(x_py, y_py)\n",
    "\n",
    "# MATLAB 等效验证\n",
    "mse_matlab = torch.mean(torch.abs(x - y)**2)\n",
    "print(\"Loss from custom function:\", loss.item())\n",
    "print(\"Equivalent MATLAB Loss:\", mse_matlab.item())\n",
    "print(\"Equivalent MATLAB Loss:\", c(x_py, y_py).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo worl\n"
     ]
    }
   ],
   "source": [
    "print('helo','worl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2435)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2435)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(3,3,3)\n",
    "y = torch.rand(3,3,3)\n",
    "a = x + 1j*y\n",
    "b = y + 1j*x\n",
    "\n",
    "\n",
    "\n",
    "loss =  torch.mean(torch.square(torch.abs(a[...]-b[...])))\n",
    "print(loss) \n",
    "diff1 = x-y\n",
    "diff2 = y-x\n",
    "torch.mean(torch.square(torch.sqrt(torch.square(diff1) + torch.square(diff2))))\n",
    "\n",
    "\n",
    "\n",
    "# disp(mean(abs(csi_perfect(:) - csi_est(:)).^2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2030, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 52, 14, 2, 2, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2191, device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Epoch [1/50]\n",
    "Epoch 1/50, Batch 50/2735, Loss: 0.4368\n",
    "Epoch 1/50, Batch 100/2735, Loss: 0.2350\n",
    "Epoch 1/50, Batch 150/2735, Loss: 0.1771\n",
    "Epoch 1/50, Batch 200/2735, Loss: 0.2383\n",
    "Epoch 1/50, Batch 250/2735, Loss: 0.1798\n",
    "Epoch 1/50, Batch 300/2735, Loss: 0.1777\n",
    "Epoch 1/50, Batch 350/2735, Loss: 0.1930\n",
    "Epoch 1/50, Batch 400/2735, Loss: 0.1769\n",
    "Epoch 1/50, Batch 450/2735, Loss: 0.1623\n",
    "Epoch 1/50, Batch 500/2735, Loss: 0.1604\n",
    "Epoch 1/50, Batch 550/2735, Loss: 0.1392\n",
    "Epoch 1/50, Batch 600/2735, Loss: 0.1530\n",
    "Epoch 1/50, Batch 650/2735, Loss: 0.1735\n",
    "Epoch 1/50, Batch 700/2735, Loss: 0.1465\n",
    "Epoch 1/50, Batch 750/2735, Loss: 0.2083\n",
    "Epoch 1/50, Batch 800/2735, Loss: 0.1294\n",
    "Epoch 1/50, Batch 850/2735, Loss: 0.1471\n",
    "Epoch 1/50, Batch 900/2735, Loss: 0.1379\n",
    "Epoch 1/50, Batch 950/2735, Loss: 0.3256\n",
    "Epoch 1/50, Batch 1000/2735, Loss: 0.1682\n",
    "Epoch 1/50, Batch 1050/2735, Loss: 0.1567\n",
    "Epoch 1/50, Batch 1100/2735, Loss: 0.1840\n",
    "Epoch 1/50, Batch 1150/2735, Loss: 0.2169\n",
    "Epoch 1/50, Batch 1200/2735, Loss: 0.1178\n",
    "Epoch 1/50, Batch 1250/2735, Loss: 0.1101\n",
    "Epoch 1/50, Batch 1300/2735, Loss: 0.1493\n",
    "Epoch 1/50, Batch 1350/2735, Loss: 0.1534\n",
    "Epoch 1/50, Batch 1400/2735, Loss: 0.1941\n",
    "Epoch 1/50, Batch 1450/2735, Loss: 0.1978\n",
    "Epoch 1/50, Batch 1500/2735, Loss: 0.2480\n",
    "Epoch 1/50, Batch 1550/2735, Loss: 0.1259\n",
    "Epoch 1/50, Batch 1600/2735, Loss: 0.1820\n",
    "Epoch 1/50, Batch 1650/2735, Loss: 0.1322\n",
    "Epoch 1/50, Batch 1700/2735, Loss: 0.1873\n",
    "Epoch 1/50, Batch 1750/2735, Loss: 0.1566\n",
    "Epoch 1/50, Batch 1800/2735, Loss: 0.2372\n",
    "Epoch 1/50, Batch 1850/2735, Loss: 0.1257\n",
    "Epoch 1/50, Batch 1900/2735, Loss: 0.2057\n",
    "Epoch 1/50, Batch 1950/2735, Loss: 0.1539\n",
    "Epoch 1/50, Batch 2000/2735, Loss: 0.1752\n",
    "Epoch 1/50, Batch 2050/2735, Loss: 0.1265\n",
    "Epoch 1/50, Batch 2100/2735, Loss: 0.2038\n",
    "Epoch 1/50, Batch 2150/2735, Loss: 0.1764\n",
    "Epoch 1/50, Batch 2200/2735, Loss: 0.1751\n",
    "Epoch 1/50, Batch 2250/2735, Loss: 0.1584\n",
    "Epoch 1/50, Batch 2300/2735, Loss: 0.1558\n",
    "Epoch 1/50, Batch 2350/2735, Loss: 0.1925\n",
    "Epoch 1/50, Batch 2400/2735, Loss: 0.2274\n",
    "Epoch 1/50, Batch 2450/2735, Loss: 0.1183\n",
    "Epoch 1/50, Batch 2500/2735, Loss: 0.1567\n",
    "Epoch 1/50, Batch 2550/2735, Loss: 0.1459\n",
    "Epoch 1/50, Batch 2600/2735, Loss: 0.1287\n",
    "Epoch 1/50, Batch 2650/2735, Loss: 0.1469\n",
    "Epoch 1/50, Batch 2700/2735, Loss: 0.2008\n",
    "Epoch 1/50, Loss: 0.1736828068519859\n",
    "Val Loss: 0.1599\n",
    "[INFO] Best model saved at epoch 1, val_loss=0.1599\n",
    "\n",
    "Epoch [2/50]\n",
    "Epoch 2/50, Batch 50/2735, Loss: 0.2030\n",
    "Epoch 2/50, Batch 100/2735, Loss: 0.1288\n",
    "Epoch 2/50, Batch 150/2735, Loss: 0.1470\n",
    "Epoch 2/50, Batch 200/2735, Loss: 0.2149\n",
    "Epoch 2/50, Batch 250/2735, Loss: 0.1298\n",
    "Epoch 2/50, Batch 300/2735, Loss: 0.1697\n",
    "Epoch 2/50, Batch 350/2735, Loss: 0.1707\n",
    "Epoch 2/50, Batch 400/2735, Loss: 0.1903\n",
    "Epoch 2/50, Batch 450/2735, Loss: 0.1589\n",
    "Epoch 2/50, Batch 500/2735, Loss: 0.1611\n",
    "Epoch 2/50, Batch 550/2735, Loss: 0.1211\n",
    "Epoch 2/50, Batch 600/2735, Loss: 0.1435\n",
    "Epoch 2/50, Batch 650/2735, Loss: 0.1529\n",
    "Epoch 2/50, Batch 700/2735, Loss: 0.1314\n",
    "Epoch 2/50, Batch 750/2735, Loss: 0.1711\n",
    "Epoch 2/50, Batch 800/2735, Loss: 0.1328\n",
    "Epoch 2/50, Batch 850/2735, Loss: 0.1233\n",
    "Epoch 2/50, Batch 900/2735, Loss: 0.1217\n",
    "Epoch 2/50, Batch 950/2735, Loss: 0.2656\n",
    "Epoch 2/50, Batch 1000/2735, Loss: 0.1350\n",
    "Epoch 2/50, Batch 1050/2735, Loss: 0.1484\n",
    "Epoch 2/50, Batch 1100/2735, Loss: 0.1530\n",
    "Epoch 2/50, Batch 1150/2735, Loss: 0.1783\n",
    "Epoch 2/50, Batch 1200/2735, Loss: 0.1020\n",
    "Epoch 2/50, Batch 1250/2735, Loss: 0.0995\n",
    "Epoch 2/50, Batch 1300/2735, Loss: 0.1462\n",
    "Epoch 2/50, Batch 1350/2735, Loss: 0.1428\n",
    "Epoch 2/50, Batch 1400/2735, Loss: 0.1774\n",
    "Epoch 2/50, Batch 1450/2735, Loss: 0.1885\n",
    "Epoch 2/50, Batch 1500/2735, Loss: 0.2170\n",
    "Epoch 2/50, Batch 1550/2735, Loss: 0.1167\n",
    "Epoch 2/50, Batch 1600/2735, Loss: 0.1698\n",
    "Epoch 2/50, Batch 1650/2735, Loss: 0.1263\n",
    "Epoch 2/50, Batch 1700/2735, Loss: 0.1908\n",
    "Epoch 2/50, Batch 1750/2735, Loss: 0.1430\n",
    "Epoch 2/50, Batch 1800/2735, Loss: 0.2342\n",
    "Epoch 2/50, Batch 1850/2735, Loss: 0.1178\n",
    "Epoch 2/50, Batch 1900/2735, Loss: 0.1513\n",
    "Epoch 2/50, Batch 1950/2735, Loss: 0.1444\n",
    "Epoch 2/50, Batch 2000/2735, Loss: 0.1670\n",
    "Epoch 2/50, Batch 2050/2735, Loss: 0.1137\n",
    "Epoch 2/50, Batch 2100/2735, Loss: 0.2001\n",
    "Epoch 2/50, Batch 2150/2735, Loss: 0.1760\n",
    "Epoch 2/50, Batch 2200/2735, Loss: 0.1725\n",
    "Epoch 2/50, Batch 2250/2735, Loss: 0.1552\n",
    "Epoch 2/50, Batch 2300/2735, Loss: 0.1519\n",
    "Epoch 2/50, Batch 2350/2735, Loss: 0.1917\n",
    "Epoch 2/50, Batch 2400/2735, Loss: 0.2209\n",
    "Epoch 2/50, Batch 2450/2735, Loss: 0.1169\n",
    "Epoch 2/50, Batch 2500/2735, Loss: 0.1375\n",
    "Epoch 2/50, Batch 2550/2735, Loss: 0.1425\n",
    "Epoch 2/50, Batch 2600/2735, Loss: 0.1256\n",
    "Epoch 2/50, Batch 2650/2735, Loss: 0.1423\n",
    "Epoch 2/50, Batch 2700/2735, Loss: 0.1913\n",
    "Epoch 2/50, Loss: 0.15338276972282522\n",
    "Val Loss: 0.1538\n",
    "[INFO] Best model saved at epoch 2, val_loss=0.1538\n",
    "\n",
    "Epoch [3/50]\n",
    "Epoch 3/50, Batch 50/2735, Loss: 0.1951\n",
    "Epoch 3/50, Batch 100/2735, Loss: 0.1170\n",
    "Epoch 3/50, Batch 150/2735, Loss: 0.1411\n",
    "Epoch 3/50, Batch 200/2735, Loss: 0.2204\n",
    "Epoch 3/50, Batch 250/2735, Loss: 0.1279\n",
    "Epoch 3/50, Batch 300/2735, Loss: 0.1603\n",
    "Epoch 3/50, Batch 350/2735, Loss: 0.1608\n",
    "Epoch 3/50, Batch 400/2735, Loss: 0.1572\n",
    "Epoch 3/50, Batch 450/2735, Loss: 0.1599\n",
    "Epoch 3/50, Batch 500/2735, Loss: 0.1548\n",
    "Epoch 3/50, Batch 550/2735, Loss: 0.1200\n",
    "Epoch 3/50, Batch 600/2735, Loss: 0.1462\n",
    "Epoch 3/50, Batch 650/2735, Loss: 0.1617\n",
    "Epoch 3/50, Batch 700/2735, Loss: 0.1319\n",
    "Epoch 3/50, Batch 750/2735, Loss: 0.1711\n",
    "Epoch 3/50, Batch 800/2735, Loss: 0.1307\n",
    "Epoch 3/50, Batch 850/2735, Loss: 0.1245\n",
    "Epoch 3/50, Batch 900/2735, Loss: 0.1215\n",
    "Epoch 3/50, Batch 950/2735, Loss: 0.2664\n",
    "Epoch 3/50, Batch 1000/2735, Loss: 0.1399\n",
    "Epoch 3/50, Batch 1050/2735, Loss: 0.1491\n",
    "Epoch 3/50, Batch 1100/2735, Loss: 0.1502\n",
    "Epoch 3/50, Batch 1150/2735, Loss: 0.1769\n",
    "Epoch 3/50, Batch 1200/2735, Loss: 0.1021\n",
    "Epoch 3/50, Batch 1250/2735, Loss: 0.0992\n",
    "Epoch 3/50, Batch 1300/2735, Loss: 0.1465\n",
    "Epoch 3/50, Batch 1350/2735, Loss: 0.1424\n",
    "Epoch 3/50, Batch 1400/2735, Loss: 0.1732\n",
    "Epoch 3/50, Batch 1450/2735, Loss: 0.1870\n",
    "Epoch 3/50, Batch 1500/2735, Loss: 0.2166\n",
    "Epoch 3/50, Batch 1550/2735, Loss: 0.1142\n",
    "Epoch 3/50, Batch 1600/2735, Loss: 0.1703\n",
    "Epoch 3/50, Batch 1650/2735, Loss: 0.1252\n",
    "Epoch 3/50, Batch 1700/2735, Loss: 0.1911\n",
    "Epoch 3/50, Batch 1750/2735, Loss: 0.1429\n",
    "Epoch 3/50, Batch 1800/2735, Loss: 0.2323\n",
    "Epoch 3/50, Batch 1850/2735, Loss: 0.1166\n",
    "Epoch 3/50, Batch 1900/2735, Loss: 0.1503\n",
    "Epoch 3/50, Batch 1950/2735, Loss: 0.1447\n",
    "Epoch 3/50, Batch 2000/2735, Loss: 0.1630\n",
    "Epoch 3/50, Batch 2050/2735, Loss: 0.1139\n",
    "Epoch 3/50, Batch 2100/2735, Loss: 0.2054\n",
    "Epoch 3/50, Batch 2150/2735, Loss: 0.1785\n",
    "Epoch 3/50, Batch 2200/2735, Loss: 0.1721\n",
    "Epoch 3/50, Batch 2250/2735, Loss: 0.1477\n",
    "Epoch 3/50, Batch 2300/2735, Loss: 0.1513\n",
    "Epoch 3/50, Batch 2350/2735, Loss: 0.1900\n",
    "Epoch 3/50, Batch 2400/2735, Loss: 0.2220\n",
    "Epoch 3/50, Batch 2450/2735, Loss: 0.1181\n",
    "Epoch 3/50, Batch 2500/2735, Loss: 0.1457\n",
    "Epoch 3/50, Batch 2550/2735, Loss: 0.1451\n",
    "Epoch 3/50, Batch 2600/2735, Loss: 0.1252\n",
    "Epoch 3/50, Batch 2650/2735, Loss: 0.1423\n",
    "Epoch 3/50, Batch 2700/2735, Loss: 0.1915\n",
    "Epoch 3/50, Loss: 0.1517968120329232\n",
    "Val Loss: 0.1553\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [y[0,:,:]]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "z.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
