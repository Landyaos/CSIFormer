{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import hdf5storage\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "# --- Native PyTorch Modules based Blocks (from previous implementation) ---\n",
    "\n",
    "class EncoderBlockNative(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Block using nn.MultiheadAttention and a custom CNN Pre-Network.\n",
    "    (Assumes pre-norm convention as implemented before)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, n_encoder_filters, kernel_size=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_encoder_filters = n_encoder_filters\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        # CNN Pre-Network\n",
    "        self.pre_conv1 = nn.Conv1d(embed_dim, n_encoder_filters, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.activation = nn.GELU()\n",
    "        self.pre_conv2 = nn.Conv1d(n_encoder_filters, n_encoder_filters, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "\n",
    "        # Residual Projection\n",
    "        if embed_dim != n_encoder_filters:\n",
    "            self.residual_proj = nn.Conv1d(embed_dim, n_encoder_filters, kernel_size=1)\n",
    "        else:\n",
    "            self.residual_proj = nn.Identity()\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(n_encoder_filters) # Applied on feature dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, embed_dim)\n",
    "        residual = x\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output, _ = self.mha(x_norm, x_norm, x_norm, need_weights=False)\n",
    "        x = residual + attn_output\n",
    "\n",
    "        residual_pre_cnn = x\n",
    "        x_norm = self.norm2(x)\n",
    "        x_permuted = x_norm.permute(0, 2, 1) # (B, embed_dim, S)\n",
    "\n",
    "        x_conv = self.pre_conv1(x_permuted)\n",
    "        x_conv = self.activation(x_conv)\n",
    "        x_conv = self.pre_conv2(x_conv) # (B, n_encoder_filters, S)\n",
    "\n",
    "        residual_permuted = residual_pre_cnn.permute(0, 2, 1)\n",
    "        residual_projected = self.residual_proj(residual_permuted)\n",
    "\n",
    "        x_out = residual_projected + x_conv\n",
    "\n",
    "        # Apply final LayerNorm (needs permutation for channel dim)\n",
    "        x_out_permuted = x_out.permute(0, 2, 1) # (B, S, n_encoder_filters)\n",
    "        x_out_norm = self.final_norm(x_out_permuted)\n",
    "        x_final = x_out_norm.permute(0, 2, 1) # (B, n_encoder_filters, S)\n",
    "        return x_final\n",
    "\n",
    "class DecoderBlockNative(nn.Module):\n",
    "    \"\"\" Residual Convolutional Block for Decoder \"\"\"\n",
    "    def __init__(self, n_decoder_filters, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_decoder_filters, n_decoder_filters, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(n_decoder_filters, n_decoder_filters, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.norm = nn.LayerNorm(n_decoder_filters) # Applied on feature dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, n_decoder_filters, seq_len)\n",
    "        residual = x\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = y + residual\n",
    "\n",
    "        # Apply LayerNorm (needs permutation for channel dim)\n",
    "        y_permuted = y.permute(0, 2, 1) # (B, S, F)\n",
    "        y_norm = self.norm(y_permuted)\n",
    "        y_final = y_norm.permute(0, 2, 1) # (B, F, S)\n",
    "        return y_final\n",
    "\n",
    "# --- Main Offline Channelformer Model (Alternative Reshape) ---\n",
    "\n",
    "class Channelformer(nn.Module):\n",
    "    def __init__(self, n_subc=224, n_sym=14, n_tx=2, n_rx=2, n_pilot_sym=14,\n",
    "                 embed_dim=128, num_heads=8, n_encoder_filters=5,\n",
    "                 n_decoder_filters=12, num_decoder_blocks=3,\n",
    "                 encoder_kernel_size=3, decoder_kernel_size=5,\n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n_subc = n_subc\n",
    "        self.n_sym = n_sym\n",
    "        self.n_tx = n_tx\n",
    "        self.n_rx = n_rx\n",
    "        self.n_pilot_sym = n_pilot_sym\n",
    "\n",
    "        # Calculate sequence lengths based on the new scheme\n",
    "        self.seq_len_pilot = n_subc * n_pilot_sym  # 72 * 2 = 144\n",
    "        self.seq_len_full = n_subc * n_sym         # 72 * 14 = 1008\n",
    "\n",
    "        # Calculate input feature dimension based on the new scheme\n",
    "        self.input_feature_dim = n_tx * n_rx * 2  # 2 * 2 * 2 = 8\n",
    "        # Calculate output feature dimension needed before final reshape\n",
    "        self.output_feature_dim = n_tx * n_rx * 2 # 8\n",
    "\n",
    "        # --- Input Projection ---\n",
    "        # Project the combined Tx-Rx-Real/Imag features to embed_dim\n",
    "        self.input_proj = nn.Linear(self.input_feature_dim, embed_dim)\n",
    "\n",
    "        # --- Encoder ---\n",
    "        # Input: (B, seq_len_pilot, embed_dim)\n",
    "        # Output: (B, n_encoder_filters, seq_len_pilot)\n",
    "        self.encoder = EncoderBlockNative(embed_dim, num_heads, n_encoder_filters,\n",
    "                                          kernel_size=encoder_kernel_size, dropout=dropout)\n",
    "\n",
    "        # --- Resizing Layer ---\n",
    "        # Maps from pilot sequence length (144) to full sequence length (1008)\n",
    "        # Project features first, then resize sequence length\n",
    "        # resize_proj: Acts on feature dim (n_encoder_filters -> n_decoder_filters)\n",
    "        # resize_seq: Acts on sequence dim (seq_len_pilot -> seq_len_full)\n",
    "        self.resize_proj = nn.Linear(n_encoder_filters, n_decoder_filters)\n",
    "        self.resize_seq = nn.Linear(self.seq_len_pilot, self.seq_len_full)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        # Input: (B, n_decoder_filters, seq_len_full)\n",
    "        # Output: (B, n_decoder_filters, seq_len_full)\n",
    "        self.decoder_blocks = nn.Sequential(\n",
    "            *[DecoderBlockNative(n_decoder_filters, kernel_size=decoder_kernel_size)\n",
    "              for _ in range(num_decoder_blocks)]\n",
    "        )\n",
    "\n",
    "        # --- Output Projection ---\n",
    "        # Maps decoder features back to the combined Tx-Rx-Real/Imag dimension (8)\n",
    "        # Use Conv1d acting on the sequence\n",
    "        self.output_proj = nn.Conv1d(n_decoder_filters, self.output_feature_dim,\n",
    "                                     kernel_size=decoder_kernel_size,\n",
    "                                     padding=decoder_kernel_size // 2)\n",
    "\n",
    "\n",
    "    def forward(self, csi_ls):\n",
    "        # Input csi_ls: (batch, n_subc, n_pilot_sym, n_tx, n_rx, 2)\n",
    "        # Example: (B, 72, 2, 2, 2, 2)\n",
    "        batch_size = csi_ls.size(0)\n",
    "\n",
    "        # 1. Reshape input according to the new scheme\n",
    "        # Target shape: (B, seq_len_pilot, input_feature_dim) = (B, 144, 8)\n",
    "        x = csi_ls.permute(0, 1, 2, 3, 4, 5).contiguous() # [B, 72, 2, 2, 2, 2]\n",
    "        x = x.view(batch_size, self.n_subc, self.n_pilot_sym, self.input_feature_dim) # [B, 72, 2, 8]\n",
    "        x = x.view(batch_size, self.seq_len_pilot, self.input_feature_dim) # [B, 144, 8]\n",
    "\n",
    "        # 2. Input Projection\n",
    "        # Input: (B, 144, 8) -> Output: (B, 144, embed_dim)\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # 3. Encoder\n",
    "        # Input: (B, 144, embed_dim) -> Output: (B, n_encoder_filters, 144)\n",
    "        encoded_features = self.encoder(x)\n",
    "\n",
    "        # 4. Resizing\n",
    "        # Permute for linear layers: (B, 144, n_encoder_filters)\n",
    "        x_resize = encoded_features.permute(0, 2, 1)\n",
    "        # Project features: (B, 144, n_decoder_filters)\n",
    "        x_resize = self.resize_proj(x_resize)\n",
    "        # Permute for sequence resize: (B, n_decoder_filters, 144)\n",
    "        x_resize = x_resize.permute(0, 2, 1)\n",
    "        # Resize sequence: (B, n_decoder_filters, seq_len_full=1008)\n",
    "        x_resized = self.resize_seq(x_resize)\n",
    "\n",
    "        # 5. Decoder\n",
    "        # Input: (B, n_decoder_filters, 1008) -> Output: (B, n_decoder_filters, 1008)\n",
    "        decoded_features = self.decoder_blocks(x_resized)\n",
    "\n",
    "        # 6. Output Projection\n",
    "        # Input: (B, n_decoder_filters, 1008) -> Output: (B, output_feature_dim=8, 1008)\n",
    "        output_seq = self.output_proj(decoded_features)\n",
    "\n",
    "        # 7. Reshape Output\n",
    "        # Reshape (B, 8, 1008) back to (B, n_subc, n_sym, n_tx, n_rx, 2)\n",
    "        # seq_len_full = n_subc * n_sym = 72 * 14 = 1008\n",
    "        # output_feature_dim = n_tx * n_rx * 2 = 8\n",
    "        output = output_seq.view(batch_size, self.n_tx, self.n_rx, 2, self.n_subc, self.n_sym)\n",
    "        # Permute dimensions to match label format: (B, n_subc, n_sym, n_tx, n_rx, 2)\n",
    "        output = output.permute(0, 4, 5, 1, 2, 3).contiguous()\n",
    "\n",
    "        return output\n",
    "\n",
    "# ##### 数据集预处理\n",
    "\n",
    "class MIMOOFDMDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csi_ls, csi_label):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param csi_ls: 导频CSI矩阵  [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :param csi: CSI矩阵 [data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        :param csi_pre: 历史CSI矩阵 [data_size, n_frame, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "        \"\"\"\n",
    "        self.csi_ls = csi_ls\n",
    "        self.csi_label = csi_label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小\"\"\"\n",
    "        return self.csi_label.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        返回单个样本\n",
    "        :param idx: 样本索引\n",
    "        :return: 发射导频、接收导频、CSI矩阵\n",
    "        \"\"\"\n",
    "        return self.csi_ls[idx], self.csi_label[idx]\n",
    "\n",
    "def dataset_preprocess(data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    csi_ls = torch.tensor(data['csiLSData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    csi_label = torch.tensor(data['csiLabelData'], dtype=torch.float32) #[data_size, n_subc, n_sym, n_tx, n_rx, 2]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return MIMOOFDMDataset(csi_ls, csi_label)\n",
    "\n",
    "\n",
    "class ComplexMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param alpha: 第一部分损失的权重\n",
    "        :param beta:  第二部分损失的权重\n",
    "        \"\"\"\n",
    "        super(ComplexMSELoss, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        复数信道估计的均方误差 (MSE) 损失函数。\n",
    "        x_py: (batch_size, csi_matrix, 2)，估计值\n",
    "        y_py: (batch_size, csi_matrix, 2)，真实值\n",
    "        \"\"\"\n",
    "        diff = output - target  # 差值，形状保持一致\n",
    "        loss = torch.mean(diff[..., 0]**2 + diff[..., 1]**2)  # 实部和虚部平方和\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir='./checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    model.to(device)\n",
    "    # 查看是否有可用的最近 checkpoint\n",
    "    latest_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_v1_latest.pth')\n",
    "    best_path = os.path.join(checkpoint_dir, model.__class__.__name__ + '_v1_best.pth')\n",
    "\n",
    "    if os.path.isfile(latest_path):\n",
    "        print(f\"[INFO] Resuming training from '{latest_path}'\")\n",
    "        checkpoint = torch.load(latest_path, map_location=device)\n",
    "\n",
    "        # 加载模型、优化器、调度器状态\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint.get('best_loss', best_loss)\n",
    "        print(f\"[INFO] Resumed epoch {start_epoch}, best_loss={best_loss:.6f}\")\n",
    "    \n",
    "    # 分epoch训练\n",
    "    print(1)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{epochs}]\")\n",
    "        # --------------------- Train ---------------------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (csi_ls_train, csi_label) in enumerate(dataloader_train):\n",
    "            csi_ls_train = csi_ls_train.to(device)\n",
    "            csi_label = csi_label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            csi_dec = model(csi_ls_train)\n",
    "            joint_loss = criterion(csi_dec, csi_label)\n",
    "            joint_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += joint_loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 2 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader_train)}, Loss: {joint_loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = total_loss / len(dataloader_train)\n",
    "        # 学习率调度器步进（根据策略）\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(train_loss)  # 对于 ReduceLROnPlateau 等需要传入指标的调度器\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader_train)}\")\n",
    "\n",
    "        # --------------------- Validate ---------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (csi_ls_val, csi_label) in enumerate(dataloader_val):\n",
    "                csi_ls_val = csi_ls_val.to(device)\n",
    "                csi_label = csi_label.to(device)\n",
    "                csi_dec = model(csi_ls_val)\n",
    "                total_loss = criterion(csi_dec, csi_label)\n",
    "                val_loss += total_loss.item()\n",
    "        \n",
    "        val_loss /= len(dataloader_val)\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # --------------------- Checkpoint 保存 ---------------------\n",
    "        # 1) 保存最新checkpoint（确保断点续训）\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "            'best_loss': best_loss,\n",
    "        }, latest_path)\n",
    "\n",
    "        # 2) 如果当前验证集 Loss 最佳，则保存为 best.pth\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'best_loss': best_loss,\n",
    "            }, best_path)\n",
    "            print(f\"[INFO] Best model saved at epoch {epoch + 1}, val_loss={val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "load done\n"
     ]
    }
   ],
   "source": [
    "print(\"load data\")\n",
    "# data_train = hdf5storage.loadmat('/root/autodl-tmp/data/raw/trainData.mat')\n",
    "# data_val = hdf5storage.loadmat('/root/autodl-tmp/data/raw/valData.mat')\n",
    "# checkpoint_dir = '/root/autodl-tmp/checkpoints'\n",
    "checkpoint_dir = './checkpoints'\n",
    "data_train = hdf5storage.loadmat('F:/dataset/valDataV5.mat')\n",
    "# data_val = hdf5storage.loadmat('F:/dataset/valDataV5.mat')\n",
    "print(\"load done\")\n",
    "dataset_train = dataset_preprocess(data_train)\n",
    "# dataset_val = dataset_preprocess(data_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 9913028\n",
      "train model\n",
      "cuda\n",
      "1\n",
      "1\n",
      "\n",
      "Epoch [1/1]\n",
      "Epoch 1, Batch 2/6000, Loss: 1.1408\n",
      "Epoch 1, Batch 4/6000, Loss: 0.8187\n",
      "Epoch 1, Batch 6/6000, Loss: 0.7356\n",
      "Epoch 1, Batch 8/6000, Loss: 0.7273\n",
      "Epoch 1, Batch 10/6000, Loss: 0.5940\n",
      "Epoch 1, Batch 12/6000, Loss: 0.7553\n",
      "Epoch 1, Batch 14/6000, Loss: 0.5971\n",
      "Epoch 1, Batch 16/6000, Loss: 0.5662\n",
      "Epoch 1, Batch 18/6000, Loss: 0.6381\n",
      "Epoch 1, Batch 20/6000, Loss: 0.4325\n",
      "Epoch 1, Batch 22/6000, Loss: 0.6605\n",
      "Epoch 1, Batch 24/6000, Loss: 0.7418\n",
      "Epoch 1, Batch 26/6000, Loss: 0.7935\n",
      "Epoch 1, Batch 28/6000, Loss: 0.8847\n",
      "Epoch 1, Batch 30/6000, Loss: 0.3396\n",
      "Epoch 1, Batch 32/6000, Loss: 0.7837\n",
      "Epoch 1, Batch 34/6000, Loss: 0.6242\n",
      "Epoch 1, Batch 36/6000, Loss: 0.4532\n",
      "Epoch 1, Batch 38/6000, Loss: 0.5644\n",
      "Epoch 1, Batch 40/6000, Loss: 0.3506\n",
      "Epoch 1, Batch 42/6000, Loss: 0.4189\n",
      "Epoch 1, Batch 44/6000, Loss: 0.4993\n",
      "Epoch 1, Batch 46/6000, Loss: 0.4535\n",
      "Epoch 1, Batch 48/6000, Loss: 0.6492\n",
      "Epoch 1, Batch 50/6000, Loss: 0.7829\n",
      "Epoch 1, Batch 52/6000, Loss: 0.4611\n",
      "Epoch 1, Batch 54/6000, Loss: 0.3992\n",
      "Epoch 1, Batch 56/6000, Loss: 0.3070\n",
      "Epoch 1, Batch 58/6000, Loss: 0.2628\n",
      "Epoch 1, Batch 60/6000, Loss: 0.6484\n",
      "Epoch 1, Batch 62/6000, Loss: 0.4401\n",
      "Epoch 1, Batch 64/6000, Loss: 0.3730\n",
      "Epoch 1, Batch 66/6000, Loss: 0.3263\n",
      "Epoch 1, Batch 68/6000, Loss: 0.6086\n",
      "Epoch 1, Batch 70/6000, Loss: 0.4389\n",
      "Epoch 1, Batch 72/6000, Loss: 0.4573\n",
      "Epoch 1, Batch 74/6000, Loss: 0.3977\n",
      "Epoch 1, Batch 76/6000, Loss: 0.6674\n",
      "Epoch 1, Batch 78/6000, Loss: 0.3553\n",
      "Epoch 1, Batch 80/6000, Loss: 0.8048\n",
      "Epoch 1, Batch 82/6000, Loss: 0.5359\n",
      "Epoch 1, Batch 84/6000, Loss: 0.4022\n",
      "Epoch 1, Batch 86/6000, Loss: 0.4403\n",
      "Epoch 1, Batch 88/6000, Loss: 0.7422\n",
      "Epoch 1, Batch 90/6000, Loss: 0.5663\n",
      "Epoch 1, Batch 92/6000, Loss: 0.3334\n",
      "Epoch 1, Batch 94/6000, Loss: 0.4632\n",
      "Epoch 1, Batch 96/6000, Loss: 0.3517\n",
      "Epoch 1, Batch 98/6000, Loss: 0.8245\n",
      "Epoch 1, Batch 100/6000, Loss: 0.4824\n",
      "Epoch 1, Batch 102/6000, Loss: 0.3625\n",
      "Epoch 1, Batch 104/6000, Loss: 0.3920\n",
      "Epoch 1, Batch 106/6000, Loss: 0.6144\n",
      "Epoch 1, Batch 108/6000, Loss: 0.5380\n",
      "Epoch 1, Batch 110/6000, Loss: 0.6773\n",
      "Epoch 1, Batch 112/6000, Loss: 0.3032\n",
      "Epoch 1, Batch 114/6000, Loss: 0.3047\n",
      "Epoch 1, Batch 116/6000, Loss: 0.2875\n",
      "Epoch 1, Batch 118/6000, Loss: 0.4914\n",
      "Epoch 1, Batch 120/6000, Loss: 0.6434\n",
      "Epoch 1, Batch 122/6000, Loss: 0.2655\n",
      "Epoch 1, Batch 124/6000, Loss: 0.3094\n",
      "Epoch 1, Batch 126/6000, Loss: 0.4002\n",
      "Epoch 1, Batch 128/6000, Loss: 0.5183\n",
      "Epoch 1, Batch 130/6000, Loss: 0.3239\n",
      "Epoch 1, Batch 132/6000, Loss: 0.5845\n",
      "Epoch 1, Batch 134/6000, Loss: 0.2817\n",
      "Epoch 1, Batch 136/6000, Loss: 0.7496\n",
      "Epoch 1, Batch 138/6000, Loss: 0.5346\n",
      "Epoch 1, Batch 140/6000, Loss: 0.4311\n",
      "Epoch 1, Batch 142/6000, Loss: 0.6582\n",
      "Epoch 1, Batch 144/6000, Loss: 0.5149\n",
      "Epoch 1, Batch 146/6000, Loss: 0.8233\n",
      "Epoch 1, Batch 148/6000, Loss: 0.4103\n",
      "Epoch 1, Batch 150/6000, Loss: 0.3512\n",
      "Epoch 1, Batch 152/6000, Loss: 0.7280\n",
      "Epoch 1, Batch 154/6000, Loss: 0.5025\n",
      "Epoch 1, Batch 156/6000, Loss: 0.3829\n",
      "Epoch 1, Batch 158/6000, Loss: 0.5395\n",
      "Epoch 1, Batch 160/6000, Loss: 0.4023\n",
      "Epoch 1, Batch 162/6000, Loss: 0.3012\n",
      "Epoch 1, Batch 164/6000, Loss: 0.5029\n",
      "Epoch 1, Batch 166/6000, Loss: 0.6621\n",
      "Epoch 1, Batch 168/6000, Loss: 0.8048\n",
      "Epoch 1, Batch 170/6000, Loss: 0.3529\n",
      "Epoch 1, Batch 172/6000, Loss: 0.3262\n",
      "Epoch 1, Batch 174/6000, Loss: 0.5874\n",
      "Epoch 1, Batch 176/6000, Loss: 0.4304\n",
      "Epoch 1, Batch 178/6000, Loss: 0.3354\n",
      "Epoch 1, Batch 180/6000, Loss: 0.3700\n",
      "Epoch 1, Batch 182/6000, Loss: 0.3959\n",
      "Epoch 1, Batch 184/6000, Loss: 0.4523\n",
      "Epoch 1, Batch 186/6000, Loss: 0.6849\n",
      "Epoch 1, Batch 188/6000, Loss: 0.3530\n",
      "Epoch 1, Batch 190/6000, Loss: 0.7229\n",
      "Epoch 1, Batch 192/6000, Loss: 0.5445\n",
      "Epoch 1, Batch 194/6000, Loss: 0.3977\n",
      "Epoch 1, Batch 196/6000, Loss: 0.4721\n",
      "Epoch 1, Batch 198/6000, Loss: 0.2444\n",
      "Epoch 1, Batch 200/6000, Loss: 0.3868\n",
      "Epoch 1, Batch 202/6000, Loss: 0.6411\n",
      "Epoch 1, Batch 204/6000, Loss: 0.5432\n",
      "Epoch 1, Batch 206/6000, Loss: 0.3443\n",
      "Epoch 1, Batch 208/6000, Loss: 0.4742\n",
      "Epoch 1, Batch 210/6000, Loss: 0.4329\n",
      "Epoch 1, Batch 212/6000, Loss: 0.7708\n",
      "Epoch 1, Batch 214/6000, Loss: 0.6137\n",
      "Epoch 1, Batch 216/6000, Loss: 0.2951\n",
      "Epoch 1, Batch 218/6000, Loss: 0.6480\n",
      "Epoch 1, Batch 220/6000, Loss: 0.5653\n",
      "Epoch 1, Batch 222/6000, Loss: 0.6374\n",
      "Epoch 1, Batch 224/6000, Loss: 0.2832\n",
      "Epoch 1, Batch 226/6000, Loss: 0.4861\n",
      "Epoch 1, Batch 228/6000, Loss: 0.3313\n",
      "Epoch 1, Batch 230/6000, Loss: 0.3899\n",
      "Epoch 1, Batch 232/6000, Loss: 0.4189\n",
      "Epoch 1, Batch 234/6000, Loss: 0.5979\n",
      "Epoch 1, Batch 236/6000, Loss: 0.5360\n",
      "Epoch 1, Batch 238/6000, Loss: 0.3095\n",
      "Epoch 1, Batch 240/6000, Loss: 0.3419\n",
      "Epoch 1, Batch 242/6000, Loss: 0.3504\n",
      "Epoch 1, Batch 244/6000, Loss: 0.4144\n",
      "Epoch 1, Batch 246/6000, Loss: 0.5822\n",
      "Epoch 1, Batch 248/6000, Loss: 0.4074\n",
      "Epoch 1, Batch 250/6000, Loss: 0.4882\n",
      "Epoch 1, Batch 252/6000, Loss: 0.4712\n",
      "Epoch 1, Batch 254/6000, Loss: 0.3188\n",
      "Epoch 1, Batch 256/6000, Loss: 0.4648\n",
      "Epoch 1, Batch 258/6000, Loss: 0.2840\n",
      "Epoch 1, Batch 260/6000, Loss: 0.5185\n",
      "Epoch 1, Batch 262/6000, Loss: 0.5279\n",
      "Epoch 1, Batch 264/6000, Loss: 0.4300\n",
      "Epoch 1, Batch 266/6000, Loss: 0.6383\n",
      "Epoch 1, Batch 268/6000, Loss: 0.2913\n",
      "Epoch 1, Batch 270/6000, Loss: 0.5060\n",
      "Epoch 1, Batch 272/6000, Loss: 0.4002\n",
      "Epoch 1, Batch 274/6000, Loss: 0.5276\n",
      "Epoch 1, Batch 276/6000, Loss: 0.4551\n",
      "Epoch 1, Batch 278/6000, Loss: 0.2431\n",
      "Epoch 1, Batch 280/6000, Loss: 0.5156\n",
      "Epoch 1, Batch 282/6000, Loss: 0.5534\n",
      "Epoch 1, Batch 284/6000, Loss: 0.5774\n",
      "Epoch 1, Batch 286/6000, Loss: 0.6203\n",
      "Epoch 1, Batch 288/6000, Loss: 0.4245\n",
      "Epoch 1, Batch 290/6000, Loss: 0.3799\n",
      "Epoch 1, Batch 292/6000, Loss: 0.4868\n",
      "Epoch 1, Batch 294/6000, Loss: 0.6758\n",
      "Epoch 1, Batch 296/6000, Loss: 0.5116\n",
      "Epoch 1, Batch 298/6000, Loss: 0.3732\n",
      "Epoch 1, Batch 300/6000, Loss: 0.3526\n",
      "Epoch 1, Batch 302/6000, Loss: 0.4507\n",
      "Epoch 1, Batch 304/6000, Loss: 0.3137\n",
      "Epoch 1, Batch 306/6000, Loss: 0.5964\n",
      "Epoch 1, Batch 308/6000, Loss: 0.3991\n",
      "Epoch 1, Batch 310/6000, Loss: 0.2403\n",
      "Epoch 1, Batch 312/6000, Loss: 0.6658\n",
      "Epoch 1, Batch 314/6000, Loss: 0.4209\n",
      "Epoch 1, Batch 316/6000, Loss: 0.2510\n",
      "Epoch 1, Batch 318/6000, Loss: 0.2469\n",
      "Epoch 1, Batch 320/6000, Loss: 0.3373\n",
      "Epoch 1, Batch 322/6000, Loss: 0.4357\n",
      "Epoch 1, Batch 324/6000, Loss: 0.2314\n",
      "Epoch 1, Batch 326/6000, Loss: 0.3068\n",
      "Epoch 1, Batch 328/6000, Loss: 0.5888\n",
      "Epoch 1, Batch 330/6000, Loss: 0.4652\n",
      "Epoch 1, Batch 332/6000, Loss: 0.2742\n",
      "Epoch 1, Batch 334/6000, Loss: 0.3630\n",
      "Epoch 1, Batch 336/6000, Loss: 0.2178\n",
      "Epoch 1, Batch 338/6000, Loss: 0.5479\n",
      "Epoch 1, Batch 340/6000, Loss: 0.4811\n",
      "Epoch 1, Batch 342/6000, Loss: 0.3148\n",
      "Epoch 1, Batch 344/6000, Loss: 0.2723\n",
      "Epoch 1, Batch 346/6000, Loss: 0.4990\n",
      "Epoch 1, Batch 348/6000, Loss: 0.3587\n",
      "Epoch 1, Batch 350/6000, Loss: 0.4505\n",
      "Epoch 1, Batch 352/6000, Loss: 0.3734\n",
      "Epoch 1, Batch 354/6000, Loss: 0.2817\n",
      "Epoch 1, Batch 356/6000, Loss: 0.4998\n",
      "Epoch 1, Batch 358/6000, Loss: 0.5584\n",
      "Epoch 1, Batch 360/6000, Loss: 0.7090\n",
      "Epoch 1, Batch 362/6000, Loss: 0.2623\n",
      "Epoch 1, Batch 364/6000, Loss: 0.3768\n",
      "Epoch 1, Batch 366/6000, Loss: 0.4619\n",
      "Epoch 1, Batch 368/6000, Loss: 0.3000\n",
      "Epoch 1, Batch 370/6000, Loss: 0.3108\n",
      "Epoch 1, Batch 372/6000, Loss: 0.4383\n",
      "Epoch 1, Batch 374/6000, Loss: 0.5719\n",
      "Epoch 1, Batch 376/6000, Loss: 0.2451\n",
      "Epoch 1, Batch 378/6000, Loss: 0.2642\n",
      "Epoch 1, Batch 380/6000, Loss: 0.2527\n",
      "Epoch 1, Batch 382/6000, Loss: 0.4263\n",
      "Epoch 1, Batch 384/6000, Loss: 0.5463\n",
      "Epoch 1, Batch 386/6000, Loss: 0.2730\n",
      "Epoch 1, Batch 388/6000, Loss: 0.3096\n",
      "Epoch 1, Batch 390/6000, Loss: 0.3770\n",
      "Epoch 1, Batch 392/6000, Loss: 0.2252\n",
      "Epoch 1, Batch 394/6000, Loss: 0.7510\n",
      "Epoch 1, Batch 396/6000, Loss: 0.2505\n",
      "Epoch 1, Batch 398/6000, Loss: 0.4158\n",
      "Epoch 1, Batch 400/6000, Loss: 0.4931\n",
      "Epoch 1, Batch 402/6000, Loss: 0.2187\n",
      "Epoch 1, Batch 404/6000, Loss: 0.1964\n",
      "Epoch 1, Batch 406/6000, Loss: 0.4001\n",
      "Epoch 1, Batch 408/6000, Loss: 0.3387\n",
      "Epoch 1, Batch 410/6000, Loss: 0.3985\n",
      "Epoch 1, Batch 412/6000, Loss: 0.2340\n",
      "Epoch 1, Batch 414/6000, Loss: 0.3636\n",
      "Epoch 1, Batch 416/6000, Loss: 0.2117\n",
      "Epoch 1, Batch 418/6000, Loss: 0.4474\n",
      "Epoch 1, Batch 420/6000, Loss: 0.1573\n",
      "Epoch 1, Batch 422/6000, Loss: 0.3624\n",
      "Epoch 1, Batch 424/6000, Loss: 0.2127\n",
      "Epoch 1, Batch 426/6000, Loss: 0.4102\n",
      "Epoch 1, Batch 428/6000, Loss: 0.2364\n",
      "Epoch 1, Batch 430/6000, Loss: 0.2073\n",
      "Epoch 1, Batch 432/6000, Loss: 0.4697\n",
      "Epoch 1, Batch 434/6000, Loss: 0.2795\n",
      "Epoch 1, Batch 436/6000, Loss: 0.2573\n",
      "Epoch 1, Batch 438/6000, Loss: 0.3369\n",
      "Epoch 1, Batch 440/6000, Loss: 0.2302\n",
      "Epoch 1, Batch 442/6000, Loss: 0.5394\n",
      "Epoch 1, Batch 444/6000, Loss: 0.3784\n",
      "Epoch 1, Batch 446/6000, Loss: 0.2430\n",
      "Epoch 1, Batch 448/6000, Loss: 0.3235\n",
      "Epoch 1, Batch 450/6000, Loss: 0.2330\n",
      "Epoch 1, Batch 452/6000, Loss: 0.2361\n",
      "Epoch 1, Batch 454/6000, Loss: 0.2833\n",
      "Epoch 1, Batch 456/6000, Loss: 0.2925\n",
      "Epoch 1, Batch 458/6000, Loss: 0.3732\n",
      "Epoch 1, Batch 460/6000, Loss: 0.3513\n",
      "Epoch 1, Batch 462/6000, Loss: 0.2802\n",
      "Epoch 1, Batch 464/6000, Loss: 0.2151\n",
      "Epoch 1, Batch 466/6000, Loss: 0.2206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler, epochs, device, checkpoint_dir)\u001b[0m\n\u001b[0;32m     37\u001b[0m joint_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 39\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mjoint_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjoint_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Channelformer()\n",
    "# 计算参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {count_parameters(model)}\")\n",
    "print('train model')\n",
    "# 主函数执行\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "lr = 1e-3\n",
    "epochs = 1\n",
    "batch_size = 1\n",
    "shuffle_flag = True\n",
    "criterion = ComplexMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "dataloader_val = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=shuffle_flag)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1)\n",
    "print(1)\n",
    "train_model(model, dataloader_train,dataloader_val, criterion, optimizer,scheduler, epochs, device, checkpoint_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
